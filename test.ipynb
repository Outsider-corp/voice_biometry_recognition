{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T13:43:25.437801Z",
     "start_time": "2024-05-05T13:43:16.169465Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import MD.main as main\n",
    "import pickle\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:43:25.448062Z",
     "start_time": "2024-05-05T13:43:25.439823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "data_path = r'{}'.format(os.environ['DATASET_PATH'])\n",
    "\n",
    "# Получение данных\n",
    "id_list = os.listdir(data_path)\n",
    "batch_size = 16\n",
    "mfcc_count = 40\n",
    "target_sr = 16000"
   ],
   "id": "2520a9b7ec9afb53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T10:34:50.055226Z",
     "start_time": "2024-05-05T10:34:50.038941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('voice_params_50pers_5_40mfccs.pkl', 'rb') as f:\n",
    "    voice_params = pickle.load(f)"
   ],
   "id": "c9b120eeb3725034",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T14:18:46.082209Z",
     "start_time": "2024-05-05T14:17:42.473314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_voices = 5\n",
    "segment_length = 0\n",
    "clear = False\n",
    "\n",
    "# Получение голосовых признаков\n",
    "voice_params = {}\n",
    "pickle_file = r'voice_params_100pers_3_40mfccs_no_segm.pkl'\n",
    "for person_id in id_list:\n",
    "    files = main.get_audio_for_id(data_path, person_id)\n",
    "    person_params = []\n",
    "    voice_cnt = 0\n",
    "    for file in files:\n",
    "        normilize_audio = main.preprocess_audio(file, target_sr=target_sr, segment_length=segment_length, clear=clear)\n",
    "        if segment_length:\n",
    "            person_params.extend(\n",
    "                [main.get_mfccs(audio, sample_rate=target_sr, n_mfcc=mfcc_count) for audio in normilize_audio])\n",
    "        else:\n",
    "            person_params.append(main.get_mfccs(normilize_audio, sample_rate=target_sr, n_mfcc=mfcc_count))\n",
    "        voice_cnt += 1\n",
    "        if voice_cnt >= max_voices:\n",
    "            break\n",
    "    voice_params[person_id] = person_params\n",
    "    print(f'Person {person_id} saved.')\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(voice_params, f)"
   ],
   "id": "a8b66e337c4718d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person id10001 saved.\n",
      "Person id10002 saved.\n",
      "Person id10003 saved.\n",
      "Person id10004 saved.\n",
      "Person id10005 saved.\n",
      "Person id10006 saved.\n",
      "Person id10007 saved.\n",
      "Person id10008 saved.\n",
      "Person id10009 saved.\n",
      "Person id10010 saved.\n",
      "Person id10011 saved.\n",
      "Person id10012 saved.\n",
      "Person id10013 saved.\n",
      "Person id10014 saved.\n",
      "Person id10015 saved.\n",
      "Person id10016 saved.\n",
      "Person id10017 saved.\n",
      "Person id10018 saved.\n",
      "Person id10019 saved.\n",
      "Person id10020 saved.\n",
      "Person id10021 saved.\n",
      "Person id10022 saved.\n",
      "Person id10023 saved.\n",
      "Person id10024 saved.\n",
      "Person id10025 saved.\n",
      "Person id10026 saved.\n",
      "Person id10027 saved.\n",
      "Person id10028 saved.\n",
      "Person id10029 saved.\n",
      "Person id10030 saved.\n",
      "Person id10031 saved.\n",
      "Person id10032 saved.\n",
      "Person id10033 saved.\n",
      "Person id10034 saved.\n",
      "Person id10035 saved.\n",
      "Person id10036 saved.\n",
      "Person id10037 saved.\n",
      "Person id10038 saved.\n",
      "Person id10039 saved.\n",
      "Person id10040 saved.\n",
      "Person id10041 saved.\n",
      "Person id10042 saved.\n",
      "Person id10043 saved.\n",
      "Person id10044 saved.\n",
      "Person id10045 saved.\n",
      "Person id10046 saved.\n",
      "Person id10047 saved.\n",
      "Person id10048 saved.\n",
      "Person id10049 saved.\n",
      "Person id10050 saved.\n",
      "Person id10051 saved.\n",
      "Person id10052 saved.\n",
      "Person id10053 saved.\n",
      "Person id10054 saved.\n",
      "Person id10055 saved.\n",
      "Person id10056 saved.\n",
      "Person id10057 saved.\n",
      "Person id10058 saved.\n",
      "Person id10059 saved.\n",
      "Person id10060 saved.\n",
      "Person id10061 saved.\n",
      "Person id10062 saved.\n",
      "Person id10063 saved.\n",
      "Person id10064 saved.\n",
      "Person id10065 saved.\n",
      "Person id10066 saved.\n",
      "Person id10067 saved.\n",
      "Person id10068 saved.\n",
      "Person id10069 saved.\n",
      "Person id10070 saved.\n",
      "Person id10071 saved.\n",
      "Person id10072 saved.\n",
      "Person id10073 saved.\n",
      "Person id10074 saved.\n",
      "Person id10075 saved.\n",
      "Person id10076 saved.\n",
      "Person id10077 saved.\n",
      "Person id10078 saved.\n",
      "Person id10079 saved.\n",
      "Person id10080 saved.\n",
      "Person id10081 saved.\n",
      "Person id10082 saved.\n",
      "Person id10083 saved.\n",
      "Person id10084 saved.\n",
      "Person id10085 saved.\n",
      "Person id10086 saved.\n",
      "Person id10087 saved.\n",
      "Person id10088 saved.\n",
      "Person id10089 saved.\n",
      "Person id10090 saved.\n",
      "Person id10091 saved.\n",
      "Person id10092 saved.\n",
      "Person id10093 saved.\n",
      "Person id10094 saved.\n",
      "Person id10095 saved.\n",
      "Person id10096 saved.\n",
      "Person id10097 saved.\n",
      "Person id10098 saved.\n",
      "Person id10099 saved.\n",
      "Person id10100 saved.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:35:03.701827Z",
     "start_time": "2024-05-05T13:35:03.693697Z"
    }
   },
   "cell_type": "code",
   "source": "voice_params['id10004'][0].shape",
   "id": "2b214704426bd768",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 286)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T14:18:49.683845Z",
     "start_time": "2024-05-05T14:18:49.670705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoiceEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_size: int = 40, channels_size: int = 128, lstm_out_size: int = 128):\n",
    "        super(VoiceEmbeddingModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, channels_size, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(channels_size)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(channels_size, channels_size, 5, padding=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # num_channels = channels_size * (128//4)\n",
    "        self.lstm1 = nn.LSTM(channels_size, lstm_out_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(lstm_out_size * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)  # Предположим, размерность эмбеддинга 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv2(x))))\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x[:, -1, :]\n",
    "        # x = self.flatten(x)\n",
    "        x = self.dropout(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VoiceEmbeddingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoiceEmbeddingCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 8, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Добавляем размерность канала\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 5 * 8)  # Выравниваем в одномерный вектор\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ],
   "id": "9a82f675d6b6b396",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T14:18:49.972818Z",
     "start_time": "2024-05-05T14:18:49.961302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Add the residual (skip) connection\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(ResCNN, self).__init__()\n",
    "\n",
    "        # Первоначальный свёрточный слой\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual блоки\n",
    "        self.res_block1 = ResidualBlock(64, 64)\n",
    "        self.res_block2 = ResidualBlock(128, 128)\n",
    "        self.res_block3 = ResidualBlock(256, 256)\n",
    "        self.res_block4 = ResidualBlock(256, 512)\n",
    "\n",
    "        # Последующие свёрточные слои\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        self.pool = nn.AvgPool2d((1, 1))\n",
    "\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "\n",
    "        # Нормализация длины\n",
    "        self.ln = nn.BatchNorm1d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Начальный свёрточный слой\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "\n",
    "        # Пропуск через рес-блоки\n",
    "        x = self.res_block1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.res_block2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.res_block3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.res_block4(x)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Полносвязный слой и нормализация длины\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        return x"
   ],
   "id": "d83d26399bfe9bad",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T14:18:50.347007Z",
     "start_time": "2024-05-05T14:18:50.320624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClippedReLU(nn.Module):\n",
    "    def __init__(self, max_value=20):\n",
    "        super(ClippedReLU, self).__init__()\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.clamp(min=0, max=self.max_value)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.clipped_relu(self.bn(x))\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.clipped_relu(x)\n",
    "        x = self.identity(x)  # Add the residual (skip) connection\n",
    "        out = self.clipped_relu(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ConvResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        in_channel = filters // 2 if filters > 64 else 1\n",
    "        super(ConvResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, filters, kernel_size=5, padding=2, stride=2)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.res_block = ResBlock(filters)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.clipped_relu(self.bn(x))\n",
    "        x = self.res_block(x)\n",
    "        x = self.res_block(x)\n",
    "        out = self.res_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepSpeakerModel(nn.Module):\n",
    "    def __init__(self, include_softmax=False):\n",
    "        super(DeepSpeakerModel, self).__init__()\n",
    "        self.include_softmax = include_softmax\n",
    "\n",
    "        # Convolution and Residual blocks setup\n",
    "        self.conv1 = ConvResBlock(64)\n",
    "        self.conv2 = ConvResBlock(128)\n",
    "        self.conv3 = ConvResBlock(256)\n",
    "        self.conv4 = ConvResBlock(512)\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Dense and output layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        if include_softmax:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        else:\n",
    "            self.norm = lambda x: F.normalize(x, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.adaptive_avg_pool(x)\n",
    "\n",
    "        # x = x.view(-1, 2048)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = torch.mean(x, dim=1) \n",
    "\n",
    "        x = self.dense1(x)\n",
    "        if self.include_softmax:\n",
    "            x = self.dropout(x)\n",
    "            x = self.output(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "        return x\n"
   ],
   "id": "90467a30bd6357f8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:54:43.072138Z",
     "start_time": "2024-05-05T15:54:43.062922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoicePairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, label = self.pairs[idx]\n",
    "        return torch.tensor(mfcc1, dtype=torch.float32).t(), torch.tensor(mfcc2, dtype=torch.float32).t(), torch.tensor(\n",
    "            label, dtype=torch.int64)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        mfcc1s, mfcc2s, labels = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "    \n",
    "        labels = torch.stack(labels).long()  # Убедитесь, что labels имеют правильный тип\n",
    "    \n",
    "        return mfcc1s_padded, mfcc2s_padded, labels\n",
    "\n",
    "\n",
    "\n",
    "class VoiceTripletsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.triplets = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, mfcc3 = self.triplets[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor(mfcc3).t()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, mfcc3 = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc3_padded = pad_sequence(mfcc3, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, mfcc3_padded"
   ],
   "id": "8cf48fb16bf2f72b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:54:44.679541Z",
     "start_time": "2024-05-05T15:54:44.663086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConstrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(ConstrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = F.pairwise_distance(anchor, positive)\n",
    "        negative_distance = F.pairwise_distance(anchor, negative)\n",
    "        losses = F.relu(positive_distance - negative_distance + self.margin)\n",
    "        return losses.mean()"
   ],
   "id": "be147e8b88d2916d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:54:45.666799Z",
     "start_time": "2024-05-05T15:54:45.662007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))"
   ],
   "id": "eaa17798701cd43c",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:55:48.156577Z",
     "start_time": "2024-05-05T15:55:48.096623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def triplet_accuracy(anchor, positive, negative):\n",
    "    pos_dist = torch.norm(anchor - positive, dim=1)\n",
    "    neg_dist = torch.norm(anchor - negative, dim=1)\n",
    "    return (pos_dist < neg_dist).float().mean()\n",
    "\n",
    "\n",
    "def pair_accuracy(output1, output2, labels):\n",
    "    margin = 1\n",
    "    distances = F.pairwise_distance(output1, output2)\n",
    "    is_correct = ((labels == 0) & (distances < margin)) | ((labels == 1) & (distances >= margin))\n",
    "    accuracy = torch.mean(is_correct.float())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_model_triplet(model: nn.Module, dataloaders: Dict, lr=0.001,\n",
    "                        epoches: int = 25, device: str = 'cuda', save_name: str = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = TripletLoss()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    stat = {}\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch + 1}/{epoches}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, inputs3 in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "                    outputs3 = model(inputs3)\n",
    "                    loss = criterion(outputs1, outputs2, outputs3)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                acc = triplet_accuracy(outputs1, outputs2, outputs3)\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "                running_corrects += acc.item() * inputs1.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stat[f'{epoch}'] = {'epoch_loss': epoch_loss,\n",
    "                                'epoch_acc': epoch_acc}\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            if save_name:\n",
    "                torch.save(model.state_dict(), os.path.join('models', f'{save_name}_epo{epoch}.pth'))\n",
    "                json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, stat\n",
    "\n",
    "\n",
    "def train_model_pair(model: nn.Module, dataloaders: Dict, lr=0.001,\n",
    "                     epoches: int = 25, device: str = 'cuda', save_name: str = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = ConstrastiveLoss()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    stat = {}\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch + 1}/{epoches}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, label in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "                    loss = criterion(outputs1, outputs2, label)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                acc = pair_accuracy(outputs1, outputs2, label)\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "                running_corrects += acc.item() * inputs1.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stat[f'{epoch}'] = {'epoch_loss': epoch_loss,\n",
    "                                'epoch_acc': epoch_acc}\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            if save_name:\n",
    "                torch.save(model.state_dict(), os.path.join('models', f'{save_name}_epo{epoch}.pth'))\n",
    "                json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, stat"
   ],
   "id": "4c9cb767b8692317",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T14:23:22.092764Z",
     "start_time": "2024-05-05T14:23:22.082937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_triplets(voice_params)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoiceTripletsDataset(data_train)\n",
    "dataset_val = VoiceTripletsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "c69089ea3d54d19e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:55:50.265899Z",
     "start_time": "2024-05-05T15:55:50.250404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_pairs(voice_params)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoicePairsDataset(data_train)\n",
    "dataset_val = VoicePairsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=VoicePairsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=VoicePairsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "3ffeb13fe9559bb6",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T16:15:55.359771Z",
     "start_time": "2024-05-05T15:55:51.204665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'DeepSpeaker_100p_5_40mfcc_002'\n",
    "model = DeepSpeakerModel()\n",
    "model, stat = train_model_pair(model, dataloaders, epoches=30, save_name=model_name)\n",
    "torch.save(model.state_dict(), os.path.join('models', f'{model_name}___end.pth'))"
   ],
   "id": "9956ac83ddec9e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.8319\n",
      "val Loss: 0.1668 Acc: 0.8277\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.8330\n",
      "val Loss: 0.1679 Acc: 0.8277\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.1470 Acc: 0.8330\n",
      "val Loss: 0.1698 Acc: 0.8277\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDeepSpeaker_100p_5_40mfcc_002\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m DeepSpeakerModel()\n\u001B[1;32m----> 3\u001B[0m model, stat \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_pair\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m___end.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[1;32mIn[52], line 137\u001B[0m, in \u001B[0;36mtrain_model_pair\u001B[1;34m(model, dataloaders, lr, epoches, device, save_name)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    136\u001B[0m     outputs1 \u001B[38;5;241m=\u001B[39m model(inputs1)\n\u001B[1;32m--> 137\u001B[0m     outputs2 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs1, outputs2, label)\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;66;03m# Обратное распространение и оптимизация только в фазе обучения\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[8], line 72\u001B[0m, in \u001B[0;36mDeepSpeakerModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     70\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)\n\u001B[0;32m     71\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x)\n\u001B[1;32m---> 72\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madaptive_avg_pool(x)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# x = x.view(-1, 2048)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[8], line 44\u001B[0m, in \u001B[0;36mConvResBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mres_block(x)\n\u001B[0;32m     43\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mres_block(x)\n\u001B[1;32m---> 44\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mres_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[8], line 21\u001B[0m, in \u001B[0;36mResBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv(x)\n\u001B[0;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclipped_relu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn(x))\n\u001B[1;32m---> 21\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn(x)\n\u001B[0;32m     23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclipped_relu(x)\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = 'DeepSpeaker_100p_5_40mfcc_003'\n",
    "model = DeepSpeakerModel()\n",
    "model, stat = train_model_triplet(model, dataloaders, epoches=50, save_name=model_name)\n",
    "torch.save(model.state_dict(), os.path.join('models', f'{model_name}___end.pth'))"
   ],
   "id": "cf92d0a5201f4644"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), f'speak_rec_20_256_128_15epo_triplets_001.pth')",
   "id": "8110f8355a947ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T15:52:37.511744Z",
     "start_time": "2024-05-05T15:52:37.146168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs1, inputs2, label in dataloaders['train']:\n",
    "    print(inputs1.shape, inputs2.shape)  # Должно соответствовать [batch_size, channels, height, width]\n",
    "    \n"
   ],
   "id": "b5ca6be843cca618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([796, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([499, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([499, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([392, 16, 40])\n",
      "torch.Size([678, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([509, 16, 40])\n",
      "torch.Size([416, 16, 40]) torch.Size([379, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([399, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([307, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([326, 16, 40]) torch.Size([531, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([397, 16, 40]) torch.Size([764, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([403, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([392, 16, 40]) torch.Size([521, 16, 40])\n",
      "torch.Size([509, 16, 40]) torch.Size([297, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([647, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([398, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([398, 16, 40])\n",
      "torch.Size([546, 16, 40]) torch.Size([628, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([394, 16, 40])\n",
      "torch.Size([647, 16, 40]) torch.Size([416, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([416, 16, 40])\n",
      "torch.Size([347, 16, 40]) torch.Size([313, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([398, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([387, 16, 40])\n",
      "torch.Size([353, 16, 40]) torch.Size([395, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([313, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([398, 16, 40])\n",
      "torch.Size([394, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([397, 16, 40]) torch.Size([317, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([448, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([414, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([1032, 16, 40])\n",
      "torch.Size([652, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([316, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([1032, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([271, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([521, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([1032, 16, 40])\n",
      "torch.Size([416, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([372, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([365, 16, 40])\n",
      "torch.Size([432, 16, 40]) torch.Size([499, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([768, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([413, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([441, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([506, 16, 40]) torch.Size([414, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([577, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([287, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([353, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([413, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([448, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([463, 16, 40]) torch.Size([521, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([318, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([364, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([567, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([647, 16, 40]) torch.Size([459, 16, 40])\n",
      "torch.Size([483, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([291, 16, 40]) torch.Size([506, 16, 40])\n",
      "torch.Size([293, 16, 40]) torch.Size([768, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([395, 16, 40])\n",
      "torch.Size([372, 16, 40]) torch.Size([392, 16, 40])\n",
      "torch.Size([316, 16, 40]) torch.Size([531, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([577, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([367, 16, 40])\n",
      "torch.Size([391, 16, 40]) torch.Size([509, 16, 40])\n",
      "torch.Size([324, 16, 40]) torch.Size([459, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([313, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([509, 16, 40])\n",
      "torch.Size([387, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([398, 16, 40])\n",
      "torch.Size([678, 16, 40]) torch.Size([678, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([414, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([359, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([326, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([546, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([567, 16, 40])\n",
      "torch.Size([317, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([395, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([441, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([567, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([334, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([459, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([364, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([499, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([426, 16, 40])\n",
      "torch.Size([395, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([364, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([401, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([289, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([463, 16, 40])\n",
      "torch.Size([638, 16, 40]) torch.Size([376, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([416, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([506, 16, 40]) torch.Size([678, 16, 40])\n",
      "torch.Size([509, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([499, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([277, 16, 40]) torch.Size([546, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([441, 16, 40]) torch.Size([577, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([506, 16, 40]) torch.Size([546, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([293, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([347, 16, 40]) torch.Size([521, 16, 40])\n",
      "torch.Size([483, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([307, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([1008, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([401, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([306, 16, 40])\n",
      "torch.Size([394, 16, 40]) torch.Size([764, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([463, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([441, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([448, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([413, 16, 40])\n",
      "torch.Size([546, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([392, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([333, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([628, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([397, 16, 40]) torch.Size([394, 16, 40])\n",
      "torch.Size([378, 16, 40]) torch.Size([414, 16, 40])\n",
      "torch.Size([432, 16, 40]) torch.Size([287, 16, 40])\n",
      "torch.Size([678, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([334, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([441, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([378, 16, 40])\n",
      "torch.Size([416, 16, 40]) torch.Size([577, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([414, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([367, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([426, 16, 40])\n",
      "torch.Size([426, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([372, 16, 40]) torch.Size([796, 16, 40])\n",
      "torch.Size([399, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([392, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([317, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([306, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([343, 16, 40]) torch.Size([307, 16, 40])\n",
      "torch.Size([338, 16, 40]) torch.Size([764, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([704, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([704, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([546, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([1032, 16, 40])\n",
      "torch.Size([416, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([387, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([312, 16, 40]) torch.Size([678, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([459, 16, 40])\n",
      "torch.Size([652, 16, 40]) torch.Size([281, 16, 40])\n",
      "torch.Size([432, 16, 40]) torch.Size([506, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([242, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([312, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([364, 16, 40]) torch.Size([463, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([271, 16, 40]) torch.Size([704, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([549, 16, 40])\n",
      "torch.Size([499, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([338, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([567, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([499, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([448, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([416, 16, 40]) torch.Size([499, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([647, 16, 40])\n",
      "torch.Size([568, 16, 40]) torch.Size([394, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([647, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([424, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([401, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([704, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([692, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([334, 16, 40]) torch.Size([413, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([351, 16, 40])\n",
      "torch.Size([483, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([961, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([359, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([678, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([596, 16, 40]) torch.Size([277, 16, 40])\n",
      "torch.Size([313, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([499, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([628, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([431, 16, 40])\n",
      "torch.Size([379, 16, 40]) torch.Size([509, 16, 40])\n",
      "torch.Size([704, 16, 40]) torch.Size([567, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([387, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([506, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([387, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([506, 16, 40]) torch.Size([376, 16, 40])\n",
      "torch.Size([343, 16, 40]) torch.Size([506, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([463, 16, 40])\n",
      "torch.Size([531, 16, 40]) torch.Size([414, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([307, 16, 40]) torch.Size([647, 16, 40])\n",
      "torch.Size([441, 16, 40]) torch.Size([437, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([506, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([353, 16, 40])\n",
      "torch.Size([796, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([326, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([343, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([387, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([453, 16, 40]) torch.Size([709, 16, 40])\n",
      "torch.Size([628, 16, 40]) torch.Size([764, 16, 40])\n",
      "torch.Size([577, 16, 40]) torch.Size([1032, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([289, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([333, 16, 40])\n",
      "torch.Size([612, 16, 40]) torch.Size([764, 16, 40])\n",
      "torch.Size([509, 16, 40]) torch.Size([453, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([365, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([716, 16, 40])\n",
      "torch.Size([546, 16, 40]) torch.Size([373, 16, 40])\n",
      "torch.Size([397, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([709, 16, 40]) torch.Size([647, 16, 40])\n",
      "torch.Size([568, 16, 40]) torch.Size([692, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([961, 16, 40])\n",
      "torch.Size([509, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([549, 16, 40]) torch.Size([403, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([549, 16, 40])\n",
      "torch.Size([373, 16, 40]) torch.Size([541, 16, 40])\n",
      "torch.Size([483, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([403, 16, 40]) torch.Size([483, 16, 40])\n",
      "torch.Size([1007, 16, 40]) torch.Size([596, 16, 40])\n",
      "torch.Size([326, 16, 40]) torch.Size([521, 16, 40])\n",
      "torch.Size([716, 16, 40]) torch.Size([399, 16, 40])\n",
      "torch.Size([359, 16, 40]) torch.Size([652, 16, 40])\n",
      "torch.Size([509, 16, 40]) torch.Size([638, 16, 40])\n",
      "torch.Size([768, 16, 40]) torch.Size([612, 16, 40])\n",
      "torch.Size([313, 16, 40]) torch.Size([647, 16, 40])\n",
      "torch.Size([1032, 16, 40]) torch.Size([1007, 16, 40])\n",
      "torch.Size([764, 16, 40]) torch.Size([432, 16, 40])\n",
      "torch.Size([395, 16, 40]) torch.Size([716, 16, 40])\n",
      "torch.Size([1008, 16, 40]) torch.Size([568, 16, 40])\n",
      "torch.Size([483, 8, 40]) torch.Size([372, 8, 40])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:07.643258Z",
     "start_time": "2024-04-17T12:32:07.540021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoiceEmbeddingModel(15).to('cuda')\n",
    "model.load_state_dict(torch.load(f'speak_rec_15_256_128_10epo.pth'))"
   ],
   "id": "f2ea0f6ef255697c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:44.371615Z",
     "start_time": "2024-04-17T12:30:44.352271Z"
    }
   },
   "cell_type": "code",
   "source": "v1 = main.get_voice_mfccs(main.get_audio_for_id(data_path, id_list[10])[0], n_mfcc=15)\n",
   "id": "b5c5ee41fb984e5c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:45.345672Z",
     "start_time": "2024-04-17T12:30:45.333727Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "3e983a4d48275e36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceEmbeddingModel(\n",
       "  (conv1): Conv1d(15, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lstm1): LSTM(128, 128, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.619731Z",
     "start_time": "2024-04-17T12:34:04.574356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma1.ogg', clear=True, clear_output=r'output/Roma1.wav',\n",
    "                             n_mfcc=15)\n",
    "roma1 = torch.tensor(roma1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma1_emb = model(roma1)"
   ],
   "id": "4a48a68983bb1778",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.915739Z",
     "start_time": "2024-04-17T12:34:04.858116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma2.ogg', clear=True, clear_output=r'output/Roma2.wav',\n",
    "                             n_mfcc=15)\n",
    "roma2 = torch.tensor(roma2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma2_emb = model(roma2)"
   ],
   "id": "6d99428b8d1efe9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:05.195946Z",
     "start_time": "2024-04-17T12:34:05.069214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad1.ogg', clear=True, clear_output=r'output/Rad1.wav',\n",
    "                            n_mfcc=15)\n",
    "rad1 = torch.tensor(rad1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad1_emb = model(rad1)"
   ],
   "id": "9f0541a18901feae",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:36:47.309960Z",
     "start_time": "2024-04-17T12:36:47.134903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad2.ogg', clear=True, clear_output=r'output/Rad2.wav',\n",
    "                            n_mfcc=15)\n",
    "rad2 = torch.tensor(rad2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad2_emb = model(rad2)"
   ],
   "id": "fccc2ae6e48d5d0a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:28.434134Z",
     "start_time": "2024-04-17T12:34:28.419158Z"
    }
   },
   "cell_type": "code",
   "source": "cosine_similarity(roma2_emb, roma1_emb)",
   "id": "233feb72037e8d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:21.436792Z",
     "start_time": "2024-04-17T12:32:21.430237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "embedding2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "cosine_similarity(embedding1, embedding2)"
   ],
   "id": "1b3f0690889b2dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9746])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:37:57.775924Z",
     "start_time": "2024-04-17T12:37:57.768285Z"
    }
   },
   "cell_type": "code",
   "source": "torch.norm(rad1_emb - roma2_emb)",
   "id": "2f9f42cb488119ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5912, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:38:10.420621Z",
     "start_time": "2024-04-17T12:38:10.402567Z"
    }
   },
   "cell_type": "code",
   "source": "rad2_emb",
   "id": "ff9f34c16aea2fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.3423,  -2.7562,  -8.6419, -27.2519,  18.2805, -27.3712,  23.5874,\n",
       "         -28.8533,  14.0961, -18.4135,  27.2655, -32.8835, -13.7695,   1.5922,\n",
       "         -13.4152,  29.1782, -32.8263,   1.3149,  20.3020,  -8.8157, -28.6114,\n",
       "           0.4546,   2.9151,  -3.2998, -28.4159,  34.9340,  -0.6875,  -8.6741,\n",
       "         -33.7749, -18.3857,  24.1713,  35.5761,  28.9477,  -5.7407, -26.5015,\n",
       "          28.8841,  21.9906, -12.5852,  -2.6302,   6.4142, -20.8082, -30.0826,\n",
       "         -29.2466,  -7.0453, -29.8049,   3.6251,  -1.7733,  23.2055,  21.0578,\n",
       "           9.0684,   6.8325, -23.0020,  22.9182,  23.0082,  -4.6133, -22.5401,\n",
       "          24.2017,  10.3489, -12.7686,  14.3108,   1.7776,  15.4300, -22.2818,\n",
       "           2.1289,  32.1716,  11.0348,  25.2061,  12.7681, -36.9366,  -1.0287,\n",
       "         -24.0626,   6.5162, -14.1568,  26.4261,  -0.5287,  16.0960,  20.7529,\n",
       "         -20.9946, -27.2554,  23.9601,  24.5917, -35.4519,   0.2399,   7.1683,\n",
       "           0.3806,  -4.1184,  18.4323, -20.9762, -20.0098,  -5.3252,   4.2149,\n",
       "          24.5013,   2.5915,  21.9206, -34.1691,   0.9695, -11.6358,  38.1673,\n",
       "           8.0780,   1.9517,   1.3659,  -0.4392,  -4.9356,  31.2155, -22.3125,\n",
       "          10.1557,  29.2507, -27.6407,  -6.5761, -24.6874, -10.3029,  13.1373,\n",
       "          28.7925,  27.7413,  27.4964,   8.2856,   4.8389,  -7.2225,  -2.5934,\n",
       "           6.9098, -32.5972,  -4.2520,   5.7384,  32.9538,   1.5417,  31.9751,\n",
       "         -12.3759,  17.3991]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:41:39.523639Z",
     "start_time": "2024-04-17T12:41:39.490994Z"
    }
   },
   "cell_type": "code",
   "source": "F.pairwise_distance(rad2_emb, roma1_emb)",
   "id": "11bdc08a41558f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9091], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:12:24.708223Z",
     "start_time": "2024-04-17T18:12:21.958105Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "63c2aa9d5fcdebc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e122b09bf0ceada1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
