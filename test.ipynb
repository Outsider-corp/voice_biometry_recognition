{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:19.297296Z",
     "start_time": "2024-05-09T09:40:09.948883Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, Dict\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import MD.main as main\n",
    "from MD.parts.clear_audio_new import clear_audio"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:19.306308Z",
     "start_time": "2024-05-09T09:40:19.298346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "data_path = r'{}'.format(os.environ['DATASET_PATH'])\n",
    "\n",
    "# Получение данных\n",
    "id_list = os.listdir(data_path)\n",
    "\n",
    "model_params = {\"persons_count\": 100,\n",
    "                \"max_voices\": 5,\n",
    "                \"mfcc_count\": 20,\n",
    "                \"batch_size\": 16,\n",
    "                \"target_sr\": 16000,\n",
    "                \"segment_length\": 0,\n",
    "                \"n_fft\": 1024,\n",
    "                \"lr\": 1e-3,\n",
    "                \"margin_triplet\": 0.3}"
   ],
   "id": "2520a9b7ec9afb53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:19.329005Z",
     "start_time": "2024-05-09T09:40:19.308354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('voice_params/voice_params_100pers_5vox_20mfcc_0segml.pkl', 'rb') as f:\n",
    "    voice_params_mfcc = pickle.load(f)"
   ],
   "id": "c9b120eeb3725034",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T23:11:50.437411Z",
     "start_time": "2024-05-08T23:11:43.647777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Получение голосовых признаков\n",
    "voice_params_mfcc = {}\n",
    "voice_params_spectro = {}\n",
    "pickle_file = f'voice_params_{model_params[\"persons_count\"]}pers_{model_params[\"max_voices\"]}vox'\n",
    "pers_i = 0\n",
    "for person_id in id_list:\n",
    "    files = main.get_audio_for_id(data_path, person_id)\n",
    "    person_params_mfcc = []\n",
    "    person_params_spectro = []\n",
    "    voice_cnt = 0\n",
    "    for file in files:\n",
    "        good_audio, good_audio_segm = clear_audio(file, target_sr=model_params['target_sr'],\n",
    "                                                  segment_length=model_params['segment_length'])\n",
    "        if good_audio_segm is not None:\n",
    "            person_params_mfcc.extend(\n",
    "                [main.get_mfccs(audio, sample_rate=model_params['target_sr'], n_mfcc=model_params['mfcc_count']) for\n",
    "                 audio in good_audio_segm])\n",
    "            # person_params_mfcc.extend(\n",
    "            #     [main.get_spectrogram(audio, target_sr) for audio in good_audio])\n",
    "        else:\n",
    "            person_params_mfcc.append(\n",
    "                main.get_mfccs(good_audio, sample_rate=model_params['target_sr'], n_mfcc=model_params['mfcc_count']))\n",
    "        person_params_spectro.append(main.get_spectrogram(good_audio, model_params['target_sr']))\n",
    "        voice_cnt += 1\n",
    "        if voice_cnt >= model_params['max_voices']:\n",
    "            break\n",
    "    voice_params_mfcc[person_id] = person_params_mfcc\n",
    "    voice_params_spectro[person_id] = person_params_spectro\n",
    "    print(f'Person {person_id} saved.')\n",
    "    pers_i += 1\n",
    "    if pers_i >= model_params['persons_count']:\n",
    "        break\n",
    "with open(os.path.join('voice_params',\n",
    "                       f'{pickle_file}_{model_params[\"mfcc_count\"]}mfcc_{model_params[\"segment_length\"]}segml.pkl'),\n",
    "          'wb') as f:\n",
    "    pickle.dump(voice_params_mfcc, f)\n",
    "with open(os.path.join('voice_params', f'{pickle_file}_spectro.pkl'), 'wb') as f:\n",
    "    pickle.dump(voice_params_spectro, f)"
   ],
   "id": "a8b66e337c4718d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person id10001 saved.\n",
      "Person id10002 saved.\n",
      "Person id10003 saved.\n",
      "Person id10004 saved.\n",
      "Person id10005 saved.\n",
      "Person id10006 saved.\n",
      "Person id10007 saved.\n",
      "Person id10008 saved.\n",
      "Person id10009 saved.\n",
      "Person id10010 saved.\n",
      "Person id10011 saved.\n",
      "Person id10012 saved.\n",
      "Person id10013 saved.\n",
      "Person id10014 saved.\n",
      "Person id10015 saved.\n",
      "Person id10016 saved.\n",
      "Person id10017 saved.\n",
      "Person id10018 saved.\n",
      "Person id10019 saved.\n",
      "Person id10020 saved.\n",
      "Person id10021 saved.\n",
      "Person id10022 saved.\n",
      "Person id10023 saved.\n",
      "Person id10024 saved.\n",
      "Person id10025 saved.\n",
      "Person id10026 saved.\n",
      "Person id10027 saved.\n",
      "Person id10028 saved.\n",
      "Person id10029 saved.\n",
      "Person id10030 saved.\n",
      "Person id10031 saved.\n",
      "Person id10032 saved.\n",
      "Person id10033 saved.\n",
      "Person id10034 saved.\n",
      "Person id10035 saved.\n",
      "Person id10036 saved.\n",
      "Person id10037 saved.\n",
      "Person id10038 saved.\n",
      "Person id10039 saved.\n",
      "Person id10040 saved.\n",
      "Person id10041 saved.\n",
      "Person id10042 saved.\n",
      "Person id10043 saved.\n",
      "Person id10044 saved.\n",
      "Person id10045 saved.\n",
      "Person id10046 saved.\n",
      "Person id10047 saved.\n",
      "Person id10048 saved.\n",
      "Person id10049 saved.\n",
      "Person id10050 saved.\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T23:45:34.159961Z",
     "start_time": "2024-05-08T23:45:34.149766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Add the residual (skip) connection\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(ResCNN, self).__init__()\n",
    "\n",
    "        # Первоначальный свёрточный слой\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual блоки\n",
    "        self.res_block1 = ResidualBlock(64, 64)\n",
    "        self.res_block2 = ResidualBlock(128, 128)\n",
    "        self.res_block3 = ResidualBlock(256, 256)\n",
    "        self.res_block4 = ResidualBlock(256, 512)\n",
    "\n",
    "        # Последующие свёрточные слои\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        self.pool = nn.AvgPool2d((1, 1))\n",
    "\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "\n",
    "        # Нормализация длины\n",
    "        self.ln = nn.BatchNorm1d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Начальный свёрточный слой\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "\n",
    "        # Пропуск через рес-блоки\n",
    "        x = self.res_block1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.res_block2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.res_block3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.res_block4(x)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Полносвязный слой и нормализация длины\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        return x"
   ],
   "id": "d83d26399bfe9bad",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:47.509638Z",
     "start_time": "2024-05-09T09:40:47.500195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClippedReLU(nn.Module):\n",
    "    def __init__(self, max_value=20):\n",
    "        super(ClippedReLU, self).__init__()\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.clamp(min=0, max=self.max_value)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        # self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv(x)\n",
    "        out = self.clipped_relu(self.bn(out))\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        # x = self.identity(x)  # Add the residual (skip) connection\n",
    "        out += identity\n",
    "        out = self.clipped_relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        in_channel = filters // 2 if filters > 64 else 1\n",
    "        super(ConvResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, filters, kernel_size=5, padding=2, stride=2)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.res_block = ResBlock(filters)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.clipped_relu(self.bn(x))\n",
    "        x = self.res_block(x)\n",
    "        x = self.res_block(x)\n",
    "        out = self.res_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepSpeakerModel(nn.Module):\n",
    "    def __init__(self, include_softmax=False):\n",
    "        super(DeepSpeakerModel, self).__init__()\n",
    "        self.include_softmax = include_softmax\n",
    "\n",
    "        # Convolution and Residual blocks setup\n",
    "        self.conv1 = ConvResBlock(64)\n",
    "        self.conv2 = ConvResBlock(128)\n",
    "        self.conv3 = ConvResBlock(256)\n",
    "        self.conv4 = ConvResBlock(512)\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Dense and output layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        if include_softmax:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        else:\n",
    "            self.norm = lambda x: F.normalize(x, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.adaptive_avg_pool(x)\n",
    "\n",
    "        # x = x.view(-1, 2048)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = torch.mean(x, dim=1) \n",
    "\n",
    "        x = self.dense1(x)\n",
    "        if self.include_softmax:\n",
    "            x = self.dropout(x)\n",
    "            # x = self.output(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "        return x\n"
   ],
   "id": "90467a30bd6357f8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:51.032143Z",
     "start_time": "2024-05-09T09:40:51.023839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoicePairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, label = self.pairs[idx]\n",
    "        return torch.tensor(mfcc1, dtype=torch.float32).t(), torch.tensor(mfcc2, dtype=torch.float32).t(), torch.tensor(\n",
    "            label, dtype=torch.int64)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        mfcc1s, mfcc2s, labels = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "        labels = torch.stack(labels).long()  # Убедитесь, что labels имеют правильный тип\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, labels\n",
    "\n",
    "\n",
    "class VoiceTripletsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.triplets = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, mfcc3 = self.triplets[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor(mfcc3).t()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, mfcc3 = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc3_padded = pad_sequence(mfcc3, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, mfcc3_padded"
   ],
   "id": "8cf48fb16bf2f72b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:55.098243Z",
     "start_time": "2024-05-09T09:40:55.093162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConstrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 0.5):\n",
    "        super(ConstrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, cos, label):\n",
    "        distance = 1 - cos\n",
    "        loss_contrastive = torch.mean(0.5 * (1 - label) * torch.pow(distance, 2) + 0.5 *\n",
    "                                      label * torch.pow(\n",
    "            F.relu(self.margin - distance), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.1):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = cosine_similarity_triplet(anchor, positive)\n",
    "        negative_distance = cosine_similarity_triplet(anchor, negative)\n",
    "        losses = F.relu(positive_distance - negative_distance + self.margin)\n",
    "        return losses.mean()"
   ],
   "id": "be147e8b88d2916d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:40:57.061477Z",
     "start_time": "2024-05-09T09:40:57.056978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity_triplet(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0), dim=1)\n",
    "\n",
    "\n",
    "def cosine_similarity_pair(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1, vec2, dim=1)"
   ],
   "id": "eaa17798701cd43c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:41:47.649410Z",
     "start_time": "2024-05-09T09:41:47.635960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def triplet_accuracy(anchor, positive, negative, margin=0.1):\n",
    "    pos_dist = cosine_similarity_triplet(anchor, positive)\n",
    "    neg_dist = cosine_similarity_triplet(anchor, negative)\n",
    "    return ((pos_dist - neg_dist) > margin).float().mean()\n",
    "\n",
    "\n",
    "def pair_accuracy(cos, labels, threshold: float = 0.5):\n",
    "    predictions = (cos >= threshold).int()\n",
    "    is_correct = (predictions == labels)\n",
    "    accuracy = torch.mean(is_correct.float())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def train_model_triplet(model: nn.Module, dataloaders: Dict,\n",
    "                        lr=0.001, epoches: int = 25,\n",
    "                        device: str = 'cuda', save_name: str = None, margin: float = 1) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = TripletLoss(margin)\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    stat = {}\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch + 1}/{epoches}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, inputs3 in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "                    outputs3 = model(inputs3)\n",
    "                    loss = criterion(outputs1, outputs2, outputs3)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                acc = triplet_accuracy(outputs1, outputs2, outputs3, margin)\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "                running_corrects += acc.item() * inputs1.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stat[f'{epoch}'] = {f'epoch_loss_{phase}': epoch_loss,\n",
    "                                f'epoch_acc_{phase}': epoch_acc}\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            if save_name:\n",
    "                torch.save(model.state_dict(), os.path.join('models', f'{save_name}_epo{epoch}.pth'))\n",
    "                json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, stat\n",
    "\n",
    "\n",
    "def train_model_pair(model: nn.Module, dataloaders: Dict, lr=0.001,\n",
    "                     epoches: int = 25, device: str = 'cuda', save_name: str = None, margin: float = 1) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = ConstrastiveLoss(margin)\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    stat = {}\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch+1}/{epoches} | time_elapsed: {time.time() - start_time}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, label in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "\n",
    "                    cos = cosine_similarity_pair(outputs1, outputs2)\n",
    "                    loss = criterion(cos, label)\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                acc = pair_accuracy(cos, label, margin)\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "                running_corrects += acc.item() * inputs1.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stat[f'{epoch}'] = {f'epoch_loss_{phase}': epoch_loss,\n",
    "                                f'epoch_acc_{phase}': epoch_acc}\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            if save_name:\n",
    "                torch.save(model.state_dict(), os.path.join('models', f'{save_name}_epo{epoch}.pth'))\n",
    "                json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "    stat['full_time'] = time.time() - start_time\n",
    "    if save_name:\n",
    "        json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, stat"
   ],
   "id": "4c9cb767b8692317",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T22:23:04.152541Z",
     "start_time": "2024-05-08T22:23:04.063302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_triplets(voice_params_mfcc, 2)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoiceTripletsDataset(data_train)\n",
    "dataset_val = VoiceTripletsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=model_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=model_params['batch_size'], shuffle=True,\n",
    "                            collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "c69089ea3d54d19e",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:41:58.527130Z",
     "start_time": "2024-05-09T09:41:58.515875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_pairs(voice_params_mfcc)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoicePairsDataset(data_train)\n",
    "dataset_val = VoicePairsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=model_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=VoicePairsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=model_params['batch_size'], shuffle=True,\n",
    "                            collate_fn=VoicePairsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "3ffeb13fe9559bb6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:44:57.584959Z",
     "start_time": "2024-05-09T09:42:23.735925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ver = '002'\n",
    "model_name = f'DeepSpeaker_{model_params[\"persons_count\"]}p_{model_params[\"max_voices\"]}vox_{model_params[\"mfcc_count\"]}mfcc_contr_{ver}'\n",
    "with open(os.path.join('models', f'params_{model_name}.json'), 'w') as f:\n",
    "    model_params.update({\"epoches\": 100})\n",
    "    json.dump(model_params, f)\n",
    "model = DeepSpeakerModel()\n",
    "model, stat = train_model_pair(model, dataloaders,\n",
    "                               epoches=model_params['epoches'],\n",
    "                               save_name=model_name,\n",
    "                               lr=model_params['lr'],\n",
    "                               margin=model_params['margin_triplet'])\n",
    "torch.save(model.state_dict(), os.path.join('models', f'{model_name}___end.pth'))"
   ],
   "id": "9956ac83ddec9e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | time_elapsed: 0.0\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.1737\n",
      "val Loss: 0.0064 Acc: 0.1454\n",
      "\n",
      "Epoch 2/100 | time_elapsed: 38.78642797470093\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.1737\n",
      "val Loss: 0.0065 Acc: 0.1454\n",
      "\n",
      "Epoch 3/100 | time_elapsed: 73.36473202705383\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.1737\n",
      "val Loss: 0.0065 Acc: 0.1454\n",
      "\n",
      "Epoch 4/100 | time_elapsed: 109.09557056427002\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.1737\n",
      "val Loss: 0.0065 Acc: 0.1454\n",
      "\n",
      "Epoch 5/100 | time_elapsed: 146.18452382087708\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m     json\u001B[38;5;241m.\u001B[39mdump(model_params, f)\n\u001B[0;32m      6\u001B[0m model \u001B[38;5;241m=\u001B[39m DeepSpeakerModel()\n\u001B[1;32m----> 7\u001B[0m model, stat \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_pair\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mepoches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepoches\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                               \u001B[49m\u001B[43msave_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mmargin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmargin_triplet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m___end.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[1;32mIn[8], line 150\u001B[0m, in \u001B[0;36mtrain_model_pair\u001B[1;34m(model, dataloaders, lr, epoches, device, save_name, margin)\u001B[0m\n\u001B[0;32m    148\u001B[0m     acc \u001B[38;5;241m=\u001B[39m pair_accuracy(cos, label, margin)\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;66;03m# Статистика\u001B[39;00m\n\u001B[1;32m--> 150\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m inputs1\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    151\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m acc\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs1\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m#     print('.', end='')\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# print()\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T22:29:48.932432Z",
     "start_time": "2024-05-08T22:23:07.663676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ver = '001'\n",
    "model_name = f'DeepSpeaker_{model_params[\"persons_count\"]}p_{model_params[\"max_voices\"]}vox_{model_params[\"mfcc_count\"]}mfcc_{ver}'\n",
    "with open(os.path.join('models', f'params_{model_name}.json'), 'w') as f:\n",
    "    model_params.update({\"epoches\": 100})\n",
    "    json.dump(model_params, f)\n",
    "model = DeepSpeakerModel(include_softmax=True).float()\n",
    "model, stat = train_model_triplet(model, dataloaders,\n",
    "                                  epoches=model_params['epoches'],\n",
    "                                  save_name=model_name,\n",
    "                                  lr=model_params['lr'],\n",
    "                                  margin=model_params['margin_triplet'])\n",
    "torch.save(model.state_dict(), os.path.join('models', f'{model_name}___end.pth'))"
   ],
   "id": "cf92d0a5201f4644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 0.6989 Acc: 0.0051\n",
      "val Loss: 0.7000 Acc: 0.0000\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 0.7146 Acc: 0.0348\n",
      "val Loss: 0.7000 Acc: 0.0000\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 0.7007 Acc: 0.0211\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 0.6929 Acc: 0.0079\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.6964 Acc: 0.0045\n",
      "val Loss: 0.7003 Acc: 0.0000\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.7020 Acc: 0.0025\n",
      "val Loss: 0.7003 Acc: 0.0000\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.7010 Acc: 0.0012\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.7007 Acc: 0.0013\n",
      "val Loss: 0.7011 Acc: 0.0000\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.6999 Acc: 0.0007\n",
      "val Loss: 0.7010 Acc: 0.0000\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.7011 Acc: 0.0017\n",
      "val Loss: 0.7010 Acc: 0.0000\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.7001 Acc: 0.0009\n",
      "val Loss: 0.7007 Acc: 0.0000\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.6999 Acc: 0.0005\n",
      "val Loss: 0.7000 Acc: 0.0000\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.7000 Acc: 0.0010\n",
      "val Loss: 0.7004 Acc: 0.0000\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.7001 Acc: 0.0009\n",
      "val Loss: 0.7001 Acc: 0.0000\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.7003 Acc: 0.0010\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.6993 Acc: 0.0011\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.7003 Acc: 0.0014\n",
      "val Loss: 0.7000 Acc: 0.0000\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.7011 Acc: 0.0015\n",
      "val Loss: 0.6999 Acc: 0.0000\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.6995 Acc: 0.0011\n",
      "val Loss: 0.7002 Acc: 0.0000\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.7000 Acc: 0.0013\n",
      "val Loss: 0.7001 Acc: 0.0000\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[92], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m     json\u001B[38;5;241m.\u001B[39mdump(model_params, f)\n\u001B[0;32m      6\u001B[0m model \u001B[38;5;241m=\u001B[39m DeepSpeakerModel(include_softmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m----> 7\u001B[0m model, stat \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_triplet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mepoches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepoches\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43msave_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m___end.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[1;32mIn[90], line 66\u001B[0m, in \u001B[0;36mtrain_model_triplet\u001B[1;34m(model, dataloaders, lr, epoches, device, save_name)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# Обратное распространение и оптимизация только в фазе обучения\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 66\u001B[0m         \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     68\u001B[0m acc \u001B[38;5;241m=\u001B[39m triplet_accuracy(outputs1, outputs2, outputs3, model_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmargin_triplet\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), f'speak_rec_20_256_128_15epo_triplets_001.pth')",
   "id": "8110f8355a947ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:07.643258Z",
     "start_time": "2024-04-17T12:32:07.540021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoiceEmbeddingModel(15).to('cuda')\n",
    "model.load_state_dict(torch.load(f'speak_rec_15_256_128_10epo.pth'))"
   ],
   "id": "f2ea0f6ef255697c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:42:15.316703Z",
     "start_time": "2024-05-09T09:42:15.310335Z"
    }
   },
   "cell_type": "code",
   "source": "len(data_pairs)",
   "id": "b5c5ee41fb984e5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5950"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:45.345672Z",
     "start_time": "2024-04-17T12:30:45.333727Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "3e983a4d48275e36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceEmbeddingModel(\n",
       "  (conv1): Conv1d(15, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lstm1): LSTM(128, 128, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.619731Z",
     "start_time": "2024-04-17T12:34:04.574356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma1.ogg', clear=True, clear_output=r'output/Roma1.wav',\n",
    "                             n_mfcc=15)\n",
    "roma1 = torch.tensor(roma1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma1_emb = model(roma1)"
   ],
   "id": "4a48a68983bb1778",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.915739Z",
     "start_time": "2024-04-17T12:34:04.858116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma2.ogg', clear=True, clear_output=r'output/Roma2.wav',\n",
    "                             n_mfcc=15)\n",
    "roma2 = torch.tensor(roma2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma2_emb = model(roma2)"
   ],
   "id": "6d99428b8d1efe9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:05.195946Z",
     "start_time": "2024-04-17T12:34:05.069214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad1.ogg', clear=True, clear_output=r'output/Rad1.wav',\n",
    "                            n_mfcc=15)\n",
    "rad1 = torch.tensor(rad1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad1_emb = model(rad1)"
   ],
   "id": "9f0541a18901feae",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:36:47.309960Z",
     "start_time": "2024-04-17T12:36:47.134903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad2.ogg', clear=True, clear_output=r'output/Rad2.wav',\n",
    "                            n_mfcc=15)\n",
    "rad2 = torch.tensor(rad2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad2_emb = model(rad2)"
   ],
   "id": "fccc2ae6e48d5d0a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:28.434134Z",
     "start_time": "2024-04-17T12:34:28.419158Z"
    }
   },
   "cell_type": "code",
   "source": "cosine_similarity(roma2_emb, roma1_emb)",
   "id": "233feb72037e8d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:21.436792Z",
     "start_time": "2024-04-17T12:32:21.430237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "embedding2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "cosine_similarity(embedding1, embedding2)"
   ],
   "id": "1b3f0690889b2dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9746])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:37:57.775924Z",
     "start_time": "2024-04-17T12:37:57.768285Z"
    }
   },
   "cell_type": "code",
   "source": "torch.norm(rad1_emb - roma2_emb)",
   "id": "2f9f42cb488119ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5912, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:38:10.420621Z",
     "start_time": "2024-04-17T12:38:10.402567Z"
    }
   },
   "cell_type": "code",
   "source": "rad2_emb",
   "id": "ff9f34c16aea2fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.3423,  -2.7562,  -8.6419, -27.2519,  18.2805, -27.3712,  23.5874,\n",
       "         -28.8533,  14.0961, -18.4135,  27.2655, -32.8835, -13.7695,   1.5922,\n",
       "         -13.4152,  29.1782, -32.8263,   1.3149,  20.3020,  -8.8157, -28.6114,\n",
       "           0.4546,   2.9151,  -3.2998, -28.4159,  34.9340,  -0.6875,  -8.6741,\n",
       "         -33.7749, -18.3857,  24.1713,  35.5761,  28.9477,  -5.7407, -26.5015,\n",
       "          28.8841,  21.9906, -12.5852,  -2.6302,   6.4142, -20.8082, -30.0826,\n",
       "         -29.2466,  -7.0453, -29.8049,   3.6251,  -1.7733,  23.2055,  21.0578,\n",
       "           9.0684,   6.8325, -23.0020,  22.9182,  23.0082,  -4.6133, -22.5401,\n",
       "          24.2017,  10.3489, -12.7686,  14.3108,   1.7776,  15.4300, -22.2818,\n",
       "           2.1289,  32.1716,  11.0348,  25.2061,  12.7681, -36.9366,  -1.0287,\n",
       "         -24.0626,   6.5162, -14.1568,  26.4261,  -0.5287,  16.0960,  20.7529,\n",
       "         -20.9946, -27.2554,  23.9601,  24.5917, -35.4519,   0.2399,   7.1683,\n",
       "           0.3806,  -4.1184,  18.4323, -20.9762, -20.0098,  -5.3252,   4.2149,\n",
       "          24.5013,   2.5915,  21.9206, -34.1691,   0.9695, -11.6358,  38.1673,\n",
       "           8.0780,   1.9517,   1.3659,  -0.4392,  -4.9356,  31.2155, -22.3125,\n",
       "          10.1557,  29.2507, -27.6407,  -6.5761, -24.6874, -10.3029,  13.1373,\n",
       "          28.7925,  27.7413,  27.4964,   8.2856,   4.8389,  -7.2225,  -2.5934,\n",
       "           6.9098, -32.5972,  -4.2520,   5.7384,  32.9538,   1.5417,  31.9751,\n",
       "         -12.3759,  17.3991]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:41:39.523639Z",
     "start_time": "2024-04-17T12:41:39.490994Z"
    }
   },
   "cell_type": "code",
   "source": "F.pairwise_distance(rad2_emb, roma1_emb)",
   "id": "11bdc08a41558f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9091], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:12:24.708223Z",
     "start_time": "2024-04-17T18:12:21.958105Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "63c2aa9d5fcdebc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e122b09bf0ceada1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
