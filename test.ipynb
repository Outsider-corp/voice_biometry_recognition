{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T13:43:25.437801Z",
     "start_time": "2024-05-05T13:43:16.169465Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import MD.main as main\n",
    "import pickle\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:43:25.448062Z",
     "start_time": "2024-05-05T13:43:25.439823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "data_path = r'{}'.format(os.environ['DATASET_PATH'])\n",
    "\n",
    "# Получение данных\n",
    "id_list = os.listdir(data_path)\n",
    "batch_size = 16\n",
    "mfcc_count = 40\n",
    "target_sr = 16000"
   ],
   "id": "2520a9b7ec9afb53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T10:34:50.055226Z",
     "start_time": "2024-05-05T10:34:50.038941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('voice_params_50pers_5_40mfccs.pkl', 'rb') as f:\n",
    "    voice_params = pickle.load(f)"
   ],
   "id": "c9b120eeb3725034",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:43:47.437343Z",
     "start_time": "2024-05-05T13:43:25.451082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_voices = 3\n",
    "segment_length = 0\n",
    "clear = True\n",
    "\n",
    "# Получение голосовых признаков\n",
    "voice_params = {}\n",
    "pickle_file = r'voice_params_100pers_3_40mfccs_no_segm.pkl'\n",
    "for person_id in id_list:\n",
    "    files = main.get_audio_for_id(data_path, person_id)\n",
    "    person_params = []\n",
    "    voice_cnt = 0\n",
    "    for file in files:\n",
    "        normilize_audio = main.preprocess_audio(file, target_sr=target_sr, segment_length=segment_length, clear=clear)\n",
    "        if segment_length:\n",
    "            person_params.extend(\n",
    "                [main.get_mfccs(audio, sample_rate=target_sr, n_mfcc=mfcc_count) for audio in normilize_audio])\n",
    "        else:\n",
    "            person_params.append(main.get_mfccs(normilize_audio, sample_rate=target_sr, n_mfcc=mfcc_count))\n",
    "        voice_cnt += 1\n",
    "        if voice_cnt >= max_voices:\n",
    "            break\n",
    "    voice_params[person_id] = person_params\n",
    "    print(f'Person {person_id} saved.')\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(voice_params, f)"
   ],
   "id": "a8b66e337c4718d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Py_Projects\\neuro\\clearing_voice\\main_project.py:355: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  power_ratio = min(power_ratio, 1 / power_ratio)\n",
      "D:\\Py_Projects\\neuro\\clearing_voice\\main_project.py:354: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  power_ratio = phrases[next_i]['power'] / phrases[i]['power']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person id10001 saved.\n",
      "Person id10002 saved.\n",
      "Person id10003 saved.\n",
      "Person id10004 saved.\n",
      "Person id10005 saved.\n",
      "Person id10006 saved.\n",
      "Person id10007 saved.\n",
      "Person id10008 saved.\n",
      "Person id10009 saved.\n",
      "Person id10010 saved.\n",
      "Person id10011 saved.\n",
      "Person id10012 saved.\n",
      "Person id10013 saved.\n",
      "Person id10014 saved.\n",
      "Person id10015 saved.\n",
      "Person id10016 saved.\n",
      "Person id10017 saved.\n",
      "Person id10018 saved.\n",
      "Person id10019 saved.\n",
      "Person id10020 saved.\n",
      "Person id10021 saved.\n",
      "Person id10022 saved.\n",
      "Person id10023 saved.\n",
      "Person id10024 saved.\n",
      "Person id10025 saved.\n",
      "Person id10026 saved.\n",
      "Person id10027 saved.\n",
      "Person id10028 saved.\n",
      "Person id10029 saved.\n",
      "Person id10030 saved.\n",
      "Person id10031 saved.\n",
      "Person id10032 saved.\n",
      "Person id10033 saved.\n",
      "Person id10034 saved.\n",
      "Person id10035 saved.\n",
      "Person id10036 saved.\n",
      "Person id10037 saved.\n",
      "Person id10038 saved.\n",
      "Person id10039 saved.\n",
      "Person id10040 saved.\n",
      "Person id10041 saved.\n",
      "Person id10042 saved.\n",
      "Person id10043 saved.\n",
      "Person id10044 saved.\n",
      "Person id10045 saved.\n",
      "Person id10046 saved.\n",
      "Person id10047 saved.\n",
      "Person id10048 saved.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (6080,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m voice_cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m files:\n\u001B[1;32m---> 13\u001B[0m     normilize_audio \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_sr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_sr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegment_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msegment_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclear\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclear\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m segment_length:\n\u001B[0;32m     15\u001B[0m         person_params\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m     16\u001B[0m             [main\u001B[38;5;241m.\u001B[39mget_mfccs(audio, sample_rate\u001B[38;5;241m=\u001B[39mtarget_sr, n_mfcc\u001B[38;5;241m=\u001B[39mmfcc_count) \u001B[38;5;28;01mfor\u001B[39;00m audio \u001B[38;5;129;01min\u001B[39;00m normilize_audio])\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\MD\\main.py:260\u001B[0m, in \u001B[0;36mpreprocess_audio\u001B[1;34m(file_path, target_sr, segment_length, clear, clear_output)\u001B[0m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;124;03mФункция для предобработки аудиозаписей.\u001B[39;00m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;124;03m:param file_path: str - Путь к аудиофайлу.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m:return: list -  Список сегментов аудиосигнала, каждый из которых является np.array.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# Загрузка аудио файла\u001B[39;00m\n\u001B[1;32m--> 260\u001B[0m y, sr \u001B[38;5;241m=\u001B[39m \u001B[43mget_voice_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclear\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclear\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclear_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclear_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;66;03m# Уменьшение шума\u001B[39;00m\n\u001B[0;32m    263\u001B[0m y_clean \u001B[38;5;241m=\u001B[39m nr\u001B[38;5;241m.\u001B[39mreduce_noise(y\u001B[38;5;241m=\u001B[39my, sr\u001B[38;5;241m=\u001B[39msr)\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\MD\\main.py:328\u001B[0m, in \u001B[0;36mget_voice_audio\u001B[1;34m(audio_path, clear, clear_output)\u001B[0m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_voice_audio\u001B[39m(audio_path: \u001B[38;5;28mstr\u001B[39m, clear: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, clear_output: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clear:\n\u001B[1;32m--> 328\u001B[0m         audio, sample_rate \u001B[38;5;241m=\u001B[39m \u001B[43mclearing_voice\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmain_project\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclear_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m                                                                     \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclear_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m         audio, sample_rate \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39mload(audio_path, sr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\clearing_voice\\main_project.py:174\u001B[0m, in \u001B[0;36mclear_audio\u001B[1;34m(input_path, output_path)\u001B[0m\n\u001B[0;32m    171\u001B[0m phrases2 \u001B[38;5;241m=\u001B[39m update_phrases(phrases, threshold_power\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m    172\u001B[0m phrases_new \u001B[38;5;241m=\u001B[39m extend_voice(phrases2, audio_clear, fs, buffer_duration\u001B[38;5;241m=\u001B[39mbuffer_duration)\n\u001B[1;32m--> 174\u001B[0m voice \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_output_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_clear\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphrases_new\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m voice_compressed \u001B[38;5;241m=\u001B[39m compress_audio(voice, phrases_new)\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_path:\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\clearing_voice\\main_project.py:459\u001B[0m, in \u001B[0;36mcreate_output_array\u001B[1;34m(x, phrases)\u001B[0m\n\u001B[0;32m    457\u001B[0m phrase_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(phrases[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart\u001B[39m\u001B[38;5;124m'\u001B[39m], phrases[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m phrases[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvoice\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m--> 459\u001B[0m     y[phrase_length] \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mphrase_length\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkaiser\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mphrases\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstart\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphrases\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbetta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;66;03m# phrase_space[phrase_length] = -1\u001B[39;00m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    463\u001B[0m     y[phrase_length] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros_like(x[phrase_length])\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (0,) (6080,) "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:35:03.701827Z",
     "start_time": "2024-05-05T13:35:03.693697Z"
    }
   },
   "cell_type": "code",
   "source": "voice_params['id10004'][0].shape",
   "id": "2b214704426bd768",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 286)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:36.601262Z",
     "start_time": "2024-05-05T12:04:36.589953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoiceEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_size: int = 40, channels_size: int = 128, lstm_out_size: int = 128):\n",
    "        super(VoiceEmbeddingModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, channels_size, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(channels_size)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(channels_size, channels_size, 5, padding=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # num_channels = channels_size * (128//4)\n",
    "        self.lstm1 = nn.LSTM(channels_size, lstm_out_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(lstm_out_size * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)  # Предположим, размерность эмбеддинга 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv2(x))))\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x[:, -1, :]\n",
    "        # x = self.flatten(x)\n",
    "        x = self.dropout(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VoiceEmbeddingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoiceEmbeddingCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 8, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Добавляем размерность канала\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 5 * 8)  # Выравниваем в одномерный вектор\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ],
   "id": "9a82f675d6b6b396",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:37.008097Z",
     "start_time": "2024-05-05T12:04:36.997262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Add the residual (skip) connection\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(ResCNN, self).__init__()\n",
    "\n",
    "        # Первоначальный свёрточный слой\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual блоки\n",
    "        self.res_block1 = ResidualBlock(64, 64)\n",
    "        self.res_block2 = ResidualBlock(128, 128)\n",
    "        self.res_block3 = ResidualBlock(256, 256)\n",
    "        self.res_block4 = ResidualBlock(256, 512)\n",
    "\n",
    "        # Последующие свёрточные слои\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        self.pool = nn.AvgPool2d((1, 1))\n",
    "\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "\n",
    "        # Нормализация длины\n",
    "        self.ln = nn.BatchNorm1d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Начальный свёрточный слой\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "\n",
    "        # Пропуск через рес-блоки\n",
    "        x = self.res_block1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.res_block2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.res_block3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.res_block4(x)\n",
    "\n",
    "        # Временной пуллинг\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Полносвязный слой и нормализация длины\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        return x"
   ],
   "id": "d83d26399bfe9bad",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:37.626786Z",
     "start_time": "2024-05-05T12:04:37.614216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClippedReLU(nn.Module):\n",
    "    def __init__(self, max_value=20):\n",
    "        super(ClippedReLU, self).__init__()\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.clamp(min=0, max=self.max_value)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.clipped_relu(self.bn(x))\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.clipped_relu(x)\n",
    "        x = self.identity(x)  # Add the residual (skip) connection\n",
    "        out = self.clipped_relu(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ConvResBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        in_channel = filters // 2 if filters > 64 else 1\n",
    "        super(ConvResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, filters, kernel_size=5, padding=2, stride=2)\n",
    "        self.clipped_relu = ClippedReLU(max_value=20)\n",
    "        self.res_block = ResBlock(filters)\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.clipped_relu(self.bn(x))\n",
    "        x = self.res_block(x)\n",
    "        x = self.res_block(x)\n",
    "        out = self.res_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepSpeakerModel(nn.Module):\n",
    "    def __init__(self, include_softmax=False):\n",
    "        super(DeepSpeakerModel, self).__init__()\n",
    "        self.include_softmax = include_softmax\n",
    "\n",
    "        # Convolution and Residual blocks setup\n",
    "        self.conv1 = ConvResBlock(64)\n",
    "        self.conv2 = ConvResBlock(128)\n",
    "        self.conv3 = ConvResBlock(256)\n",
    "        self.conv4 = ConvResBlock(512)\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Dense and output layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        if include_softmax:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        else:\n",
    "            self.norm = lambda x: F.normalize(x, p=2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.adaptive_avg_pool(x)\n",
    "\n",
    "        # x = x.view(-1, 2048)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = torch.mean(x, dim=1) \n",
    "\n",
    "        x = self.dense1(x)\n",
    "        if self.include_softmax:\n",
    "            x = self.dropout(x)\n",
    "            x = self.output(x)\n",
    "            x = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "        return x\n"
   ],
   "id": "90467a30bd6357f8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:37.954114Z",
     "start_time": "2024-05-05T12:04:37.945798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoicePairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, label = self.pairs[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor([label])\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, labels = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, labels\n",
    "\n",
    "\n",
    "class VoiceTripletsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.triplets = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, mfcc3 = self.triplets[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor(mfcc3).t()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, mfcc3 = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "        mfcc3_padded = pad_sequence(mfcc3, batch_first=True, padding_value=0).transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, mfcc3_padded"
   ],
   "id": "8cf48fb16bf2f72b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:38.314051Z",
     "start_time": "2024-05-05T12:04:38.309341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConstrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(ConstrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 0.5) +\n",
    "                                      (label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 0.5))\n",
    "        loss_contrastive *= 1000\n",
    "        return loss_contrastive\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = F.pairwise_distance(anchor, positive)\n",
    "        negative_distance = F.pairwise_distance(anchor, negative)\n",
    "        losses = F.relu(positive_distance - negative_distance + self.margin)\n",
    "        return losses.mean()"
   ],
   "id": "be147e8b88d2916d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:39.359858Z",
     "start_time": "2024-05-05T12:04:39.352204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))"
   ],
   "id": "eaa17798701cd43c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:28:40.779116Z",
     "start_time": "2024-05-05T13:28:40.753025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def triplet_accuracy(anchor, positive, negative):\n",
    "    pos_dist = torch.norm(anchor - positive, dim=1)\n",
    "    neg_dist = torch.norm(anchor - negative, dim=1)\n",
    "    return (pos_dist < neg_dist).float().mean()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, dataloaders: Dict, criterion, lr=0.001,\n",
    "                epoches: int = 25, device: str = 'cuda', save_name: str = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    stat = {}\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch + 1}/{epoches}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, inputs3 in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "                    outputs3 = model(inputs3)\n",
    "                    loss = criterion(outputs1, outputs2, outputs3)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                acc = triplet_accuracy(outputs1, outputs2, outputs3)\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "                running_corrects += acc.item() * inputs1.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stat[f'{epoch}'] = {'epoch_loss': epoch_loss,\n",
    "                                'epoch_acc': epoch_acc}\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "                if save_name:\n",
    "                    torch.save(model.state_dict(), os.path.join('models', f'{save_name}_epo{epoch}.pth'))\n",
    "                    json.dump(stat, open(os.path.join('models', f'{save_name}.json'), 'w'))\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, stat"
   ],
   "id": "4c9cb767b8692317",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:28:41.020714Z",
     "start_time": "2024-05-05T13:28:41.008140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_triplets(voice_params)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoiceTripletsDataset(data_train)\n",
    "dataset_val = VoiceTripletsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "c69089ea3d54d19e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ffeb13fe9559bb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T13:29:33.644523Z",
     "start_time": "2024-05-05T13:28:41.440159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'DeepSpeaker_100p_3_40mfcc_001'\n",
    "model = DeepSpeakerModel()\n",
    "model, stat = train_model(model, dataloaders, TripletLoss(), epoches=50, save_name=model_name)\n",
    "torch.save(model.state_dict(), os.path.join('models', f'{model_name}___end.pth'))"
   ],
   "id": "9956ac83ddec9e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.9895 Acc: 0.5250\n",
      "val Loss: 1.0018 Acc: 0.4000\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.8058 Acc: 0.6000\n",
      "val Loss: 1.0599 Acc: 0.5500\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.8442 Acc: 0.4750\n",
      "val Loss: 0.9913 Acc: 0.5000\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.8841 Acc: 0.6750\n",
      "val Loss: 0.9724 Acc: 0.6000\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.8139 Acc: 0.6875\n",
      "val Loss: 0.9288 Acc: 0.5500\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.8147 Acc: 0.7500\n",
      "val Loss: 0.9481 Acc: 0.6500\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.8074 Acc: 0.8750\n",
      "val Loss: 0.9439 Acc: 0.6500\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.7953 Acc: 0.6375\n",
      "val Loss: 0.8605 Acc: 0.5500\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.8466 Acc: 0.7500\n",
      "val Loss: 1.0149 Acc: 0.4500\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.7746 Acc: 0.6500\n",
      "val Loss: 0.9588 Acc: 0.8000\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.8553 Acc: 0.5875\n",
      "val Loss: 0.9659 Acc: 0.8500\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.7893 Acc: 0.7000\n",
      "val Loss: 0.9974 Acc: 1.0000\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.7711 Acc: 0.7000\n",
      "val Loss: 0.9986 Acc: 1.0000\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.8388 Acc: 0.7000\n",
      "val Loss: 0.9995 Acc: 1.0000\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.8839 Acc: 0.4375\n",
      "val Loss: 0.9992 Acc: 1.0000\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.9362 Acc: 0.6000\n",
      "val Loss: 1.0006 Acc: 0.0000\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.9131 Acc: 0.3625\n",
      "val Loss: 1.0013 Acc: 0.0000\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.8111 Acc: 0.7750\n",
      "val Loss: 1.0017 Acc: 0.0000\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.7855 Acc: 0.5750\n",
      "val Loss: 0.9996 Acc: 1.0000\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.6661 Acc: 0.6750\n",
      "val Loss: 0.9996 Acc: 1.0000\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.7734 Acc: 0.6250\n",
      "val Loss: 0.9984 Acc: 1.0000\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.7309 Acc: 0.5750\n",
      "val Loss: 0.9996 Acc: 1.0000\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.7453 Acc: 0.7375\n",
      "val Loss: 0.9996 Acc: 1.0000\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.7424 Acc: 0.7125\n",
      "val Loss: 0.9994 Acc: 1.0000\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.7441 Acc: 0.5500\n",
      "val Loss: 0.9999 Acc: 0.8000\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.7042 Acc: 0.6125\n",
      "val Loss: 1.0006 Acc: 0.0500\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.7305 Acc: 0.5875\n",
      "val Loss: 0.9991 Acc: 0.9000\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.7902 Acc: 0.7000\n",
      "val Loss: 1.0020 Acc: 0.0000\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.9584 Acc: 0.6000\n",
      "val Loss: 0.9997 Acc: 0.9500\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.8797 Acc: 0.5625\n",
      "val Loss: 1.0004 Acc: 0.0000\n",
      "\n",
      "Лучшая точность валидации: 1.0000\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), f'speak_rec_20_256_128_15epo_triplets_001.pth')",
   "id": "8110f8355a947ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T10:40:08.326763Z",
     "start_time": "2024-05-05T10:40:08.321729Z"
    }
   },
   "cell_type": "code",
   "source": "len(voice_params['id10004'])",
   "id": "b5ca6be843cca618",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:07.643258Z",
     "start_time": "2024-04-17T12:32:07.540021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoiceEmbeddingModel(15).to('cuda')\n",
    "model.load_state_dict(torch.load(f'speak_rec_15_256_128_10epo.pth'))"
   ],
   "id": "f2ea0f6ef255697c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:44.371615Z",
     "start_time": "2024-04-17T12:30:44.352271Z"
    }
   },
   "cell_type": "code",
   "source": "v1 = main.get_voice_mfccs(main.get_audio_for_id(data_path, id_list[10])[0], n_mfcc=15)\n",
   "id": "b5c5ee41fb984e5c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:45.345672Z",
     "start_time": "2024-04-17T12:30:45.333727Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "3e983a4d48275e36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceEmbeddingModel(\n",
       "  (conv1): Conv1d(15, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lstm1): LSTM(128, 128, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.619731Z",
     "start_time": "2024-04-17T12:34:04.574356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma1.ogg', clear=True, clear_output=r'output/Roma1.wav',\n",
    "                             n_mfcc=15)\n",
    "roma1 = torch.tensor(roma1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma1_emb = model(roma1)"
   ],
   "id": "4a48a68983bb1778",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.915739Z",
     "start_time": "2024-04-17T12:34:04.858116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma2.ogg', clear=True, clear_output=r'output/Roma2.wav',\n",
    "                             n_mfcc=15)\n",
    "roma2 = torch.tensor(roma2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma2_emb = model(roma2)"
   ],
   "id": "6d99428b8d1efe9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:05.195946Z",
     "start_time": "2024-04-17T12:34:05.069214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad1.ogg', clear=True, clear_output=r'output/Rad1.wav',\n",
    "                            n_mfcc=15)\n",
    "rad1 = torch.tensor(rad1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad1_emb = model(rad1)"
   ],
   "id": "9f0541a18901feae",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:36:47.309960Z",
     "start_time": "2024-04-17T12:36:47.134903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad2.ogg', clear=True, clear_output=r'output/Rad2.wav',\n",
    "                            n_mfcc=15)\n",
    "rad2 = torch.tensor(rad2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad2_emb = model(rad2)"
   ],
   "id": "fccc2ae6e48d5d0a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:28.434134Z",
     "start_time": "2024-04-17T12:34:28.419158Z"
    }
   },
   "cell_type": "code",
   "source": "cosine_similarity(roma2_emb, roma1_emb)",
   "id": "233feb72037e8d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:21.436792Z",
     "start_time": "2024-04-17T12:32:21.430237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "embedding2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "cosine_similarity(embedding1, embedding2)"
   ],
   "id": "1b3f0690889b2dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9746])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:37:57.775924Z",
     "start_time": "2024-04-17T12:37:57.768285Z"
    }
   },
   "cell_type": "code",
   "source": "torch.norm(rad1_emb - roma2_emb)",
   "id": "2f9f42cb488119ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5912, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:38:10.420621Z",
     "start_time": "2024-04-17T12:38:10.402567Z"
    }
   },
   "cell_type": "code",
   "source": "rad2_emb",
   "id": "ff9f34c16aea2fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.3423,  -2.7562,  -8.6419, -27.2519,  18.2805, -27.3712,  23.5874,\n",
       "         -28.8533,  14.0961, -18.4135,  27.2655, -32.8835, -13.7695,   1.5922,\n",
       "         -13.4152,  29.1782, -32.8263,   1.3149,  20.3020,  -8.8157, -28.6114,\n",
       "           0.4546,   2.9151,  -3.2998, -28.4159,  34.9340,  -0.6875,  -8.6741,\n",
       "         -33.7749, -18.3857,  24.1713,  35.5761,  28.9477,  -5.7407, -26.5015,\n",
       "          28.8841,  21.9906, -12.5852,  -2.6302,   6.4142, -20.8082, -30.0826,\n",
       "         -29.2466,  -7.0453, -29.8049,   3.6251,  -1.7733,  23.2055,  21.0578,\n",
       "           9.0684,   6.8325, -23.0020,  22.9182,  23.0082,  -4.6133, -22.5401,\n",
       "          24.2017,  10.3489, -12.7686,  14.3108,   1.7776,  15.4300, -22.2818,\n",
       "           2.1289,  32.1716,  11.0348,  25.2061,  12.7681, -36.9366,  -1.0287,\n",
       "         -24.0626,   6.5162, -14.1568,  26.4261,  -0.5287,  16.0960,  20.7529,\n",
       "         -20.9946, -27.2554,  23.9601,  24.5917, -35.4519,   0.2399,   7.1683,\n",
       "           0.3806,  -4.1184,  18.4323, -20.9762, -20.0098,  -5.3252,   4.2149,\n",
       "          24.5013,   2.5915,  21.9206, -34.1691,   0.9695, -11.6358,  38.1673,\n",
       "           8.0780,   1.9517,   1.3659,  -0.4392,  -4.9356,  31.2155, -22.3125,\n",
       "          10.1557,  29.2507, -27.6407,  -6.5761, -24.6874, -10.3029,  13.1373,\n",
       "          28.7925,  27.7413,  27.4964,   8.2856,   4.8389,  -7.2225,  -2.5934,\n",
       "           6.9098, -32.5972,  -4.2520,   5.7384,  32.9538,   1.5417,  31.9751,\n",
       "         -12.3759,  17.3991]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:41:39.523639Z",
     "start_time": "2024-04-17T12:41:39.490994Z"
    }
   },
   "cell_type": "code",
   "source": "F.pairwise_distance(rad2_emb, roma1_emb)",
   "id": "11bdc08a41558f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9091], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:12:24.708223Z",
     "start_time": "2024-04-17T18:12:21.958105Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "63c2aa9d5fcdebc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e122b09bf0ceada1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
