{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T08:11:08.319656Z",
     "start_time": "2024-04-18T08:10:54.495427Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import MD.main as main\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T08:11:08.327678Z",
     "start_time": "2024-04-18T08:11:08.320664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "data_path = r'{}'.format(os.environ['DATASET_PATH'])\n",
    "\n",
    "# Получение данных\n",
    "id_list = os.listdir(data_path)\n",
    "batch_size = 256\n",
    "mfcc_count = 20\n",
    "target_sr = 16000"
   ],
   "id": "2520a9b7ec9afb53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:05:11.037121Z",
     "start_time": "2024-04-17T23:05:10.983999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('voice_params_20pers_20mfccs_with_normalize.pkl', 'rb') as f:\n",
    "    voice_params = pickle.load(f)"
   ],
   "id": "c9b120eeb3725034",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T08:16:06.809333Z",
     "start_time": "2024-04-18T08:11:19.593735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Получение голосовых признаков\n",
    "voice_params = {}\n",
    "pickle_file = r'voice_params_20pers_20mfccs_with_normalize.pkl'\n",
    "for person_id in id_list:\n",
    "    files = main.get_audio_for_id(data_path, person_id)\n",
    "    person_params = []\n",
    "    for file in files:\n",
    "        normilize_audio = main.preprocess_audio(file, target_sr=target_sr, segment_length=3)\n",
    "        person_params.extend([main.get_mfccs(audio, sample_rate=target_sr, n_mfcc=mfcc_count) for audio in normilize_audio])\n",
    "    voice_params[person_id] = person_params\n",
    "    print(f'Person {person_id} saved.')\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(voice_params, f)"
   ],
   "id": "a8b66e337c4718d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person id10001 saved.\n",
      "Person id10002 saved.\n",
      "Person id10003 saved.\n",
      "Person id10004 saved.\n",
      "Person id10005 saved.\n",
      "Person id10006 saved.\n",
      "Person id10007 saved.\n",
      "Person id10008 saved.\n",
      "Person id10009 saved.\n",
      "Person id10010 saved.\n",
      "Person id10011 saved.\n",
      "Person id10012 saved.\n",
      "Person id10013 saved.\n",
      "Person id10014 saved.\n",
      "Person id10015 saved.\n",
      "Person id10016 saved.\n",
      "Person id10017 saved.\n",
      "Person id10018 saved.\n",
      "Person id10019 saved.\n",
      "Person id10020 saved.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:10:51.043638Z",
     "start_time": "2024-04-17T23:10:51.032818Z"
    }
   },
   "cell_type": "code",
   "source": "voice_params['id10011'][10].shape",
   "id": "2b214704426bd768",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:17:17.180841Z",
     "start_time": "2024-04-17T23:17:17.170794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoiceEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_size: int = 40, channels_size: int = 128, lstm_out_size: int = 128):\n",
    "        super(VoiceEmbeddingModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, channels_size, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(channels_size)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(channels_size, channels_size, 5, padding=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # num_channels = channels_size * (128//4)\n",
    "        self.lstm1 = nn.LSTM(channels_size, lstm_out_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(lstm_out_size * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)  # Предположим, размерность эмбеддинга 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(nn.functional.relu(self.bn1(self.conv2(x))))\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x[:, -1, :]\n",
    "        # x = self.flatten(x)\n",
    "        x = self.dropout(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class VoiceEmbeddingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoiceEmbeddingCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 8, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Добавляем размерность канала\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 5 * 8)  # Выравниваем в одномерный вектор\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ],
   "id": "9a82f675d6b6b396",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:17:14.200173Z",
     "start_time": "2024-04-17T23:17:14.193879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoicePairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, label = self.pairs[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor([label])\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, labels = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, labels\n",
    "\n",
    "\n",
    "class VoiceTripletsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.triplets = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc1, mfcc2, mfcc3 = self.triplets[idx]\n",
    "        return torch.tensor(mfcc1).t(), torch.tensor(mfcc2).t(), torch.tensor(mfcc3).t()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch) -> Tuple:\n",
    "        mfcc1s, mfcc2s, mfcc3 = zip(*batch)\n",
    "        mfcc1s_padded = pad_sequence(mfcc1s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "        mfcc2s_padded = pad_sequence(mfcc2s, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "        mfcc3_padded = pad_sequence(mfcc3, batch_first=True, padding_value=0).transpose(1, 2)\n",
    "\n",
    "        return mfcc1s_padded, mfcc2s_padded, mfcc3_padded"
   ],
   "id": "8cf48fb16bf2f72b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:17:20.413530Z",
     "start_time": "2024-04-17T23:17:20.407534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConstrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(ConstrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 0.5) +\n",
    "                                      (label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 0.5))\n",
    "        loss_contrastive *= 1000\n",
    "        return loss_contrastive\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = F.pairwise_distance(anchor, positive)\n",
    "        negative_distance = F.pairwise_distance(anchor, negative)\n",
    "        losses = F.relu(positive_distance - negative_distance + self.margin)\n",
    "        return losses.mean()"
   ],
   "id": "be147e8b88d2916d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:17:22.471897Z",
     "start_time": "2024-04-17T23:17:22.467415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))"
   ],
   "id": "eaa17798701cd43c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:17:27.983505Z",
     "start_time": "2024-04-17T23:17:27.975438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model: nn.Module, dataloaders: Dict, criterion, lr=0.001,\n",
    "                epoches: int = 25, device: str = 'cuda') -> nn.Module:\n",
    "    \"\"\"\n",
    "    Обучает модель и выводит информацию о процессе обучения.\n",
    "   :param model: torch.nn.Module -  Модель для обучения.\n",
    "   :param dataloaders: dict - Словарь содержащий 'train' и 'val' DataLoader.\n",
    "   :param criterion: torch.nn.modules.loss - Функция потерь.\n",
    "   :param lr: float\n",
    "   :param epoches: int - Количество эпох обучения.\n",
    "   :param device: str - Устройство для обучения ('cuda' или 'cpu').\n",
    "    :return: nn.Module - Обученная модель.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Epoch {epoch + 1}/{epoches}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установка модели в режим обучения\n",
    "            else:\n",
    "                model.eval()  # Установка модели в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным.\n",
    "            for inputs1, inputs2, inputs3 in dataloaders[phase]:\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "\n",
    "                # Обнуление градиентов параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1 = model(inputs1)\n",
    "                    outputs2 = model(inputs2)\n",
    "                    outputs3 = model(inputs3)\n",
    "                    loss = criterion(outputs1, outputs2, outputs3)\n",
    "\n",
    "                    # Обратное распространение и оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Копирование модели, если она показала лучшую точность\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "        print()\n",
    "    print(f'Лучшая точность валидации: {best_acc:.4f}')\n",
    "\n",
    "    # Загрузка лучших весов модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "id": "4c9cb767b8692317",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:18:36.838579Z",
     "start_time": "2024-04-17T23:17:37.804325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_pairs = main.create_triplets(voice_params)\n",
    "random.shuffle(data_pairs)\n",
    "# Разделение на обучающую и валидационную выборки\n",
    "val_size = int(0.2 * len(data_pairs))\n",
    "data_train = data_pairs[val_size:]\n",
    "data_val = data_pairs[:val_size]\n",
    "\n",
    "dataset_train = VoiceTripletsDataset(data_train)\n",
    "dataset_val = VoiceTripletsDataset(data_val)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=VoiceTripletsDataset.collate_fn)\n",
    "dataloaders = {'train': dataloader_train,\n",
    "               'val': dataloader_val}"
   ],
   "id": "c69089ea3d54d19e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T00:15:37.227475Z",
     "start_time": "2024-04-17T23:18:57.189889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoiceEmbeddingCNN()\n",
    "model = train_model(model, dataloaders, TripletLoss(), epoches=10)\n",
    "torch.save(model.state_dict(), f'speak_rec_20_256_128_15epo_triplets_002.pth')"
   ],
   "id": "9956ac83ddec9e03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m VoiceEmbeddingCNN()\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTripletLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspeak_rec_20_256_128_15epo_triplets_002.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 32\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataloaders, criterion, lr, epoches, device)\u001B[0m\n\u001B[0;32m     29\u001B[0m running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Итерация по данным.\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs3\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[43mphase\u001B[49m\u001B[43m]\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minputs1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minputs2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[10], line 39\u001B[0m, in \u001B[0;36mVoiceTripletsDataset.collate_fn\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     37\u001B[0m mfcc1s_padded \u001B[38;5;241m=\u001B[39m pad_sequence(mfcc1s, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     38\u001B[0m mfcc2s_padded \u001B[38;5;241m=\u001B[39m pad_sequence(mfcc2s, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 39\u001B[0m mfcc3_padded \u001B[38;5;241m=\u001B[39m \u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmfcc3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mfcc1s_padded, mfcc2s_padded, mfcc3_padded\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:399\u001B[0m, in \u001B[0;36mpad_sequence\u001B[1;34m(sequences, batch_first, padding_value)\u001B[0m\n\u001B[0;32m    395\u001B[0m         sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    397\u001B[0m \u001B[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001B[39;00m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001B[39;00m\n\u001B[1;32m--> 399\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), f'speak_rec_20_256_128_15epo_triplets_001.pth')",
   "id": "8110f8355a947ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:07.643258Z",
     "start_time": "2024-04-17T12:32:07.540021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoiceEmbeddingModel(15).to('cuda')\n",
    "model.load_state_dict(torch.load(f'speak_rec_15_256_128_10epo.pth'))"
   ],
   "id": "f2ea0f6ef255697c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T14:37:39.225916Z",
     "start_time": "2024-04-17T14:37:39.181234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for a, b, c in dataloader_train:\n",
    "    batch = (a, b, c)\n",
    "    break"
   ],
   "id": "eb585c9314dafbf8",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [283, 20] at entry 0 and [197, 20] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataloader_train\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mbreak\u001B[39;49;00m\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Py_Projects\\neuro\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[21], line 18\u001B[0m, in \u001B[0;36mVoicePairsDataset.collate_fn\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     15\u001B[0m mfcc1s_padded \u001B[38;5;241m=\u001B[39m pad_sequence(mfcc1s, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     16\u001B[0m mfcc2s_padded \u001B[38;5;241m=\u001B[39m pad_sequence(mfcc2s, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mfcc1s_padded, mfcc2s_padded, labels\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [283, 20] at entry 0 and [197, 20] at entry 1"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:44.371615Z",
     "start_time": "2024-04-17T12:30:44.352271Z"
    }
   },
   "cell_type": "code",
   "source": "v1 = main.get_voice_mfccs(main.get_audio_for_id(data_path, id_list[10])[0], n_mfcc=15)\n",
   "id": "b5c5ee41fb984e5c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:30:45.345672Z",
     "start_time": "2024-04-17T12:30:45.333727Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "3e983a4d48275e36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VoiceEmbeddingModel(\n",
       "  (conv1): Conv1d(15, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lstm1): LSTM(128, 128, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.619731Z",
     "start_time": "2024-04-17T12:34:04.574356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma1.ogg', clear=True, clear_output=r'output/Roma1.wav',\n",
    "                             n_mfcc=15)\n",
    "roma1 = torch.tensor(roma1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma1_emb = model(roma1)"
   ],
   "id": "4a48a68983bb1778",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:04.915739Z",
     "start_time": "2024-04-17T12:34:04.858116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "roma2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Roma2.ogg', clear=True, clear_output=r'output/Roma2.wav',\n",
    "                             n_mfcc=15)\n",
    "roma2 = torch.tensor(roma2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "roma2_emb = model(roma2)"
   ],
   "id": "6d99428b8d1efe9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:05.195946Z",
     "start_time": "2024-04-17T12:34:05.069214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad1 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad1.ogg', clear=True, clear_output=r'output/Rad1.wav',\n",
    "                            n_mfcc=15)\n",
    "rad1 = torch.tensor(rad1, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad1_emb = model(rad1)"
   ],
   "id": "9f0541a18901feae",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:36:47.309960Z",
     "start_time": "2024-04-17T12:36:47.134903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rad2 = main.get_voice_mfccs(r'D:\\University\\Диссерт\\val_data\\Rad2.ogg', clear=True, clear_output=r'output/Rad2.wav',\n",
    "                            n_mfcc=15)\n",
    "rad2 = torch.tensor(rad2, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
    "rad2_emb = model(rad2)"
   ],
   "id": "fccc2ae6e48d5d0a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:15.484081Z",
     "start_time": "2024-04-17T12:32:15.460323Z"
    }
   },
   "cell_type": "code",
   "source": "rad1_emb",
   "id": "77f1b5a3f8acd14d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.0123,  -2.2304,  -8.5127, -27.8939,  18.5335, -27.9814,  24.6854,\n",
       "         -29.6719,  14.2017, -18.7953,  27.9606, -33.8146, -13.9903,   0.9342,\n",
       "         -13.6428,  30.0798, -33.7939,   1.9956,  20.6993,  -8.6234, -29.2436,\n",
       "           1.1553,   2.4182,  -2.7796, -29.0598,  36.0511,  -1.2846,  -8.5150,\n",
       "         -35.1723, -18.6078,  25.3065,  36.7703,  29.6898,  -6.5807, -27.0767,\n",
       "          29.6256,  22.5598, -12.4937,  -3.4577,   6.7036, -21.6700, -30.9447,\n",
       "         -29.9405,  -6.7488, -30.6491,   4.3517,  -2.4798,  23.6850,  21.4136,\n",
       "           9.1717,   6.6804, -23.5795,  23.3602,  23.4931,  -5.4534, -23.1248,\n",
       "          25.0040,  10.1666, -12.6213,  15.0363,   1.2163,  15.8564, -22.6861,\n",
       "           2.8412,  33.3507,  10.9805,  25.6619,  12.7693, -38.1582,  -0.4479,\n",
       "         -24.5357,   6.2112, -14.3070,  27.1434,   0.1968,  16.3499,  21.7442,\n",
       "         -21.3088, -28.4449,  25.0744,  25.1865, -36.4896,   0.8688,   6.8741,\n",
       "           1.0673,  -4.8335,  18.6386, -21.3199, -20.4204,  -5.1818,   3.7273,\n",
       "          24.9844,   2.0757,  22.7593, -35.1946,   0.3284, -11.5238,  39.3446,\n",
       "           7.8021,   2.5611,   0.8029,   0.1650,  -4.4558,  31.9747, -22.8966,\n",
       "          10.2097,  29.9952, -28.2245,  -6.2084, -25.8677, -10.2757,  13.2702,\n",
       "          29.9962,  28.3338,  28.5811,   8.1466,   4.4397,  -7.9426,  -3.3190,\n",
       "           7.4458, -33.7176,  -4.9395,   6.6018,  33.8653,   0.9501,  32.8393,\n",
       "         -12.2855,  17.7660]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:34:28.434134Z",
     "start_time": "2024-04-17T12:34:28.419158Z"
    }
   },
   "cell_type": "code",
   "source": "cosine_similarity(roma2_emb, roma1_emb)",
   "id": "233feb72037e8d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:32:21.436792Z",
     "start_time": "2024-04-17T12:32:21.430237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "embedding2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "cosine_similarity(embedding1, embedding2)"
   ],
   "id": "1b3f0690889b2dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9746])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:37:57.775924Z",
     "start_time": "2024-04-17T12:37:57.768285Z"
    }
   },
   "cell_type": "code",
   "source": "torch.norm(rad1_emb - roma2_emb)",
   "id": "2f9f42cb488119ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5912, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:38:10.420621Z",
     "start_time": "2024-04-17T12:38:10.402567Z"
    }
   },
   "cell_type": "code",
   "source": "rad2_emb",
   "id": "ff9f34c16aea2fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.3423,  -2.7562,  -8.6419, -27.2519,  18.2805, -27.3712,  23.5874,\n",
       "         -28.8533,  14.0961, -18.4135,  27.2655, -32.8835, -13.7695,   1.5922,\n",
       "         -13.4152,  29.1782, -32.8263,   1.3149,  20.3020,  -8.8157, -28.6114,\n",
       "           0.4546,   2.9151,  -3.2998, -28.4159,  34.9340,  -0.6875,  -8.6741,\n",
       "         -33.7749, -18.3857,  24.1713,  35.5761,  28.9477,  -5.7407, -26.5015,\n",
       "          28.8841,  21.9906, -12.5852,  -2.6302,   6.4142, -20.8082, -30.0826,\n",
       "         -29.2466,  -7.0453, -29.8049,   3.6251,  -1.7733,  23.2055,  21.0578,\n",
       "           9.0684,   6.8325, -23.0020,  22.9182,  23.0082,  -4.6133, -22.5401,\n",
       "          24.2017,  10.3489, -12.7686,  14.3108,   1.7776,  15.4300, -22.2818,\n",
       "           2.1289,  32.1716,  11.0348,  25.2061,  12.7681, -36.9366,  -1.0287,\n",
       "         -24.0626,   6.5162, -14.1568,  26.4261,  -0.5287,  16.0960,  20.7529,\n",
       "         -20.9946, -27.2554,  23.9601,  24.5917, -35.4519,   0.2399,   7.1683,\n",
       "           0.3806,  -4.1184,  18.4323, -20.9762, -20.0098,  -5.3252,   4.2149,\n",
       "          24.5013,   2.5915,  21.9206, -34.1691,   0.9695, -11.6358,  38.1673,\n",
       "           8.0780,   1.9517,   1.3659,  -0.4392,  -4.9356,  31.2155, -22.3125,\n",
       "          10.1557,  29.2507, -27.6407,  -6.5761, -24.6874, -10.3029,  13.1373,\n",
       "          28.7925,  27.7413,  27.4964,   8.2856,   4.8389,  -7.2225,  -2.5934,\n",
       "           6.9098, -32.5972,  -4.2520,   5.7384,  32.9538,   1.5417,  31.9751,\n",
       "         -12.3759,  17.3991]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T12:41:39.523639Z",
     "start_time": "2024-04-17T12:41:39.490994Z"
    }
   },
   "cell_type": "code",
   "source": "F.pairwise_distance(rad2_emb, roma1_emb)",
   "id": "11bdc08a41558f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9091], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:12:24.708223Z",
     "start_time": "2024-04-17T18:12:21.958105Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "63c2aa9d5fcdebc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e122b09bf0ceada1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
