from __future__ import print_function, division
import os

import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from speechnas.nas_core.global_enum import Activation
from speechnas.nas_core.layer.common_layer.conv1d import Conv1d_
from speechnas.nas_core.layer.common_layer.dtdnn import TransitLayer, DenseLayer
from speechnas.nas_core.layer.common_layer.dtdnnss import DTDNNSS_
from speechnas.nas_core.layer.common_layer.tdnn import TDNN_, StatisticsPooling

LEN_FRAME = 200


class DtdnnssBase(nn.Module):
    def __init__(self, num_class, mid_channels=128, feature_dim=512, loss_type='aam_loss'):
        super(DtdnnssBase, self).__init__()
        self.loss_type = loss_type

        # TODO Layer 1
        self.layer1 = TDNN_(in_channels=20, out_channels=128, kernel_size=5, bias=True,
                            dilation=1, in_shape=LEN_FRAME, out_shape=LEN_FRAME,
                            act=Activation.PReLU)

        # TODO Layer 2
        self.layer2 = DTDNNSS_(in_channels=128, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 3
        self.layer3 = DTDNNSS_(in_channels=192, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 4
        self.layer4 = DTDNNSS_(in_channels=256, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 5
        self.layer5 = DTDNNSS_(in_channels=320, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 6
        self.layer6 = DTDNNSS_(in_channels=384, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 7
        self.layer7 = DTDNNSS_(in_channels=448, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 8
        self.layer8 = TransitLayer(in_channels=512, out_channels=256, act=Activation.PReLU,
                                   in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 9
        self.layer9 = DTDNNSS_(in_channels=256, mid_channels=mid_channels, out_channels=64,
                               kernel_size=3,
                               dilation=(1, 3), bias=False, in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO Layer 10
        self.layer10 = DTDNNSS_(in_channels=320, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 11
        self.layer11 = DTDNNSS_(in_channels=384, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 12
        self.layer12 = DTDNNSS_(in_channels=448, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 13
        self.layer13 = DTDNNSS_(in_channels=512, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 14
        self.layer14 = DTDNNSS_(in_channels=576, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 15
        self.layer15 = DTDNNSS_(in_channels=640, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 16
        self.layer16 = DTDNNSS_(in_channels=704, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 17
        self.layer17 = DTDNNSS_(in_channels=768, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 18
        self.layer18 = DTDNNSS_(in_channels=832, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 19
        self.layer19 = DTDNNSS_(in_channels=896, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 20
        self.layer20 = DTDNNSS_(in_channels=960, mid_channels=mid_channels, out_channels=64,
                                kernel_size=3,
                                dilation=(1, 3), bias=False, in_shape=LEN_FRAME,
                                out_shape=LEN_FRAME)

        # TODO Layer 21
        self.layer21 = TransitLayer(in_channels=1024, out_channels=512, act=Activation.PReLU,
                                    in_shape=LEN_FRAME, out_shape=LEN_FRAME)

        # TODO pooling 22
        self.pooling22 = StatisticsPooling(512)

        # TODO Layer 23
        self.layer23 = DenseLayer(in_channels=512 * 2, out_channels=feature_dim, in_shape=1,
                                  out_shape=1)

        # TODO Layer logits
        if self.loss_type == 'aam_loss':
            self.weight = torch.nn.Parameter(torch.randn(num_class, feature_dim, 1),
                                             requires_grad=True)
        else:
            self.logits = Conv1d_(feature_dim, num_class, 1, act=Activation.Identity, in_shape=1,
                                  out_shape=1)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1. / n))
                # m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm1d):
                if m.weight is not None:
                    m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                # m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()
        if self.loss_type == 'aam_loss':
            torch.nn.init.normal_(self.weight, 0., 0.01)  # It seems better.

    def forward(self, inputs, **kwargs):
        # TODO Layer 1
        feature1 = self.layer1(inputs)
        # TODO Layer 2
        feature2 = self.layer2(feature1)
        # TODO Layer 3
        feature3 = self.layer3(feature2)
        # TODO Layer 4
        feature4 = self.layer4(feature3)
        # TODO Layer 5
        feature5 = self.layer5(feature4)
        # TODO Layer 6
        feature6 = self.layer6(feature5)
        # TODO Layer 7
        feature7 = self.layer7(feature6)
        # TODO Layer 8
        feature8 = self.layer8(feature7)
        # TODO Layer 9
        feature9 = self.layer9(feature8)
        # TODO Layer 10
        feature10 = self.layer10(feature9)
        # TODO Layer 11
        feature11 = self.layer11(feature10)
        # TODO Layer 12
        feature12 = self.layer12(feature11)
        # TODO Layer 13
        feature13 = self.layer13(feature12)
        # TODO Layer 14
        feature14 = self.layer14(feature13)
        # TODO Layer 15
        feature15 = self.layer15(feature14)
        # TODO Layer 16
        feature16 = self.layer16(feature15)
        # TODO Layer 17
        feature17 = self.layer17(feature16)
        # TODO Layer 18
        feature18 = self.layer18(feature17)
        # TODO Layer 19
        feature19 = self.layer19(feature18)
        # TODO Layer 20
        feature20 = self.layer20(feature19)
        # TODO Layer 21
        feature21 = self.layer21(feature20)
        # TODO Layer 22
        feature22 = self.pooling22(feature21)
        # TODO Layer 23
        feature23 = self.layer23(feature22)

        # get madds
        self.madds = 0
        self.param = 0
        self.madds += self.layer1.multiply_adds
        self.madds += self.layer2.multiply_adds
        self.madds += self.layer3.multiply_adds
        self.madds += self.layer4.multiply_adds
        self.madds += self.layer5.multiply_adds
        self.madds += self.layer6.multiply_adds
        self.madds += self.layer7.multiply_adds
        self.madds += self.layer8.multiply_adds
        self.madds += self.layer9.multiply_adds
        self.madds += self.layer10.multiply_adds
        self.madds += self.layer11.multiply_adds
        self.madds += self.layer12.multiply_adds
        self.madds += self.layer13.multiply_adds
        self.madds += self.layer14.multiply_adds
        self.madds += self.layer15.multiply_adds
        self.madds += self.layer16.multiply_adds
        self.madds += self.layer17.multiply_adds
        self.madds += self.layer18.multiply_adds
        self.madds += self.layer19.multiply_adds
        self.madds += self.layer20.multiply_adds
        self.madds += self.layer21.multiply_adds
        # self.madds += self.pooling22.multiply_adds
        self.madds += self.layer23.multiply_adds

        # get params
        self.param += self.layer1.params
        self.param += self.layer2.params
        self.param += self.layer3.params
        self.param += self.layer4.params
        self.param += self.layer5.params
        self.param += self.layer6.params
        self.param += self.layer7.params
        self.param += self.layer8.params
        self.param += self.layer9.params
        self.param += self.layer10.params
        self.param += self.layer11.params
        self.param += self.layer12.params
        self.param += self.layer13.params
        self.param += self.layer14.params
        self.param += self.layer15.params
        self.param += self.layer16.params
        self.param += self.layer17.params
        self.param += self.layer18.params
        self.param += self.layer19.params
        self.param += self.layer20.params
        self.param += self.layer21.params
        # self.param += self.pooling22.params
        self.param += self.layer23.params

        # TODO logits
        if self.loss_type == 'aam_loss':
            normalized_x = F.normalize(feature23.squeeze(dim=2), dim=1)
            normalized_weight = F.normalize(self.weight.squeeze(dim=2), dim=1)
            cosine_theta = F.linear(normalized_x, normalized_weight)  # Y = W*X
            return cosine_theta
        else:
            result = self.logits(feature23)
            self.madds += self.logits.multiply_adds
            self.param += self.logits.params
            return result

    def get_madds(self):
        return self.madds

    def get_param(self):
        return self.param

    @for_extract_embedding(maxChunk=10000, isMatrix=True)
    def extract_embedding(self, inputs):
        # TODO Layer 1
        feature1 = self.layer1(inputs)
        # TODO Layer 2
        feature2 = self.layer2(feature1)
        # TODO Layer 3
        feature3 = self.layer3(feature2)
        # TODO Layer 4
        feature4 = self.layer4(feature3)
        # TODO Layer 5
        feature5 = self.layer5(feature4)
        # TODO Layer 6
        feature6 = self.layer6(feature5)
        # TODO Layer 7
        feature7 = self.layer7(feature6)
        # TODO Layer 8
        feature8 = self.layer8(feature7)
        # TODO Layer 9
        feature9 = self.layer9(feature8)
        # TODO Layer 10
        feature10 = self.layer10(feature9)
        # TODO Layer 11
        feature11 = self.layer11(feature10)
        # TODO Layer 12
        feature12 = self.layer12(feature11)
        # TODO Layer 13
        feature13 = self.layer13(feature12)
        # TODO Layer 14
        feature14 = self.layer14(feature13)
        # TODO Layer 15
        feature15 = self.layer15(feature14)
        # TODO Layer 16
        feature16 = self.layer16(feature15)
        # TODO Layer 17
        feature17 = self.layer17(feature16)
        # TODO Layer 18
        feature18 = self.layer18(feature17)
        # TODO Layer 19
        feature19 = self.layer19(feature18)
        # TODO Layer 20
        feature20 = self.layer20(feature19)
        # TODO Layer 21
        feature21 = self.layer21(feature20)
        # TODO Layer 22
        feature22 = self.pooling22(feature21)
        # TODO Layer 23
        feature23 = self.layer23(feature22)

        # get madds
        self.madds = 0
        self.param = 0
        self.madds += self.layer1.multiply_adds
        self.madds += self.layer2.multiply_adds
        self.madds += self.layer3.multiply_adds
        self.madds += self.layer4.multiply_adds
        self.madds += self.layer5.multiply_adds
        self.madds += self.layer6.multiply_adds
        self.madds += self.layer7.multiply_adds
        self.madds += self.layer8.multiply_adds
        self.madds += self.layer9.multiply_adds
        self.madds += self.layer10.multiply_adds
        self.madds += self.layer11.multiply_adds
        self.madds += self.layer12.multiply_adds
        self.madds += self.layer13.multiply_adds
        self.madds += self.layer14.multiply_adds
        self.madds += self.layer15.multiply_adds
        self.madds += self.layer16.multiply_adds
        self.madds += self.layer17.multiply_adds
        self.madds += self.layer18.multiply_adds
        self.madds += self.layer19.multiply_adds
        self.madds += self.layer20.multiply_adds
        self.madds += self.layer21.multiply_adds
        # self.madds += self.pooling22.multiply_adds
        self.madds += self.layer23.multiply_adds

        # get params
        self.param += self.layer1.params
        self.param += self.layer2.params
        self.param += self.layer3.params
        self.param += self.layer4.params
        self.param += self.layer5.params
        self.param += self.layer6.params
        self.param += self.layer7.params
        self.param += self.layer8.params
        self.param += self.layer9.params
        self.param += self.layer10.params
        self.param += self.layer11.params
        self.param += self.layer12.params
        self.param += self.layer13.params
        self.param += self.layer14.params
        self.param += self.layer15.params
        self.param += self.layer16.params
        self.param += self.layer17.params
        self.param += self.layer18.params
        self.param += self.layer19.params
        self.param += self.layer20.params
        self.param += self.layer21.params
        # self.param += self.pooling22.params
        self.param += self.layer23.params

        return feature23

class TDNN_(nn.Module):
    def __init__(self, in_channels,
                 out_channels,
                 kernel_size,
                 stride=1,
                 padding=Padding.SAME,
                 dilation=1,
                 groups=1,
                 bias=False,
                 act=Activation.ReLU,
                 reduction=4,
                 is_use_skip_connect=False,
                 is_folded=False,
                 in_shape=None,
                 out_shape=None
                 ):
        super(TDNN_, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding_type = padding
        self.dilation = dilation
        self.groups = groups
        self.bias = bias
        self.act_type = act
        self.is_use_skip_connect = is_use_skip_connect
        self.is_folded = is_folded
        self.padding = padding
        self.in_shape = in_shape
        self.out_shape = out_shape
        self.conv = Conv1d_(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation,
            groups=self.groups,
            bias=self.bias,
            act=Activation.Identity,
            is_folded=is_folded,
            in_shape=in_shape,
            out_shape=out_shape
        )
        self.bn = BatchNorm1d_(num_features=self.out_channels,
                               momentum=0.1,
                               affine=True,
                               act=Activation.Identity,
                               is_folded=is_folded)
        if self.act_type == Activation.PReLU:
            self.act = _act(self.act_type, num_parameters=self.out_channels)
        else:
            self.act = _act(self.act_type)

    def forward(self, inputs):
        result = self.conv(inputs)
        result = self.bn(result)
        result = self.act(result)
        if self.is_use_skip_connect is True and self.in_channels == self.out_channels:
            result = result + inputs
        return result

    @property
    def multiply_adds(self):
        madds = self.conv.multiply_adds
        madds += self.bn.multiply_adds
        return madds

    @property
    def params(self):
        params = self.conv.params
        params += self.bn.params
        if self.act_type == Activation.PReLU:
            params += self.out_channels
        return params


class DTDNNSS_(nn.Module):
    def __init__(self, in_channels,
                 mid_channels,
                 out_channels,
                 kernel_size,
                 stride=1,
                 padding=Padding.SAME,
                 dilation=None,
                 groups=1,
                 bias=False,
                 act=Activation.PReLU,
                 is_folded=False,
                 in_shape=None,
                 out_shape=None,
                 null=False,
                 reduction=2
                 ):
        super(DTDNNSS_, self).__init__()
        self.in_channels = in_channels
        self.mid_channels = mid_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding_type = padding
        self.dilation = dilation
        self.groups = groups
        self.bias = bias
        self.act_type = act
        self.is_folded = is_folded
        self.padding = padding
        self.in_shape = in_shape
        self.out_shape = out_shape
        self.bn1 = BatchNorm1d_(num_features=self.in_channels,
                                momentum=0.1,
                                affine=True,
                                act=Activation.Identity,
                                is_folded=is_folded)
        self.act1 = _act(self.act_type, self.in_channels)
        self.conv1 = Conv1d_(
            in_channels=self.in_channels,
            out_channels=self.mid_channels,
            kernel_size=1,
            stride=self.stride,
            padding=self.padding,
            dilation=1,
            groups=self.groups,
            bias=self.bias,
            act=Activation.Identity,
            is_folded=is_folded,
            in_shape=in_shape,
            out_shape=out_shape
        )

        self.bn2 = BatchNorm1d_(num_features=self.mid_channels,
                                momentum=0.1,
                                affine=True,
                                act=Activation.Identity,
                                is_folded=is_folded)
        self.act2 = _act(self.act_type, self.mid_channels)
        self.conv2 = nn.ModuleList()
        for _dilation in self.dilation:
            self.conv2.append(
                Conv1d_(
                    in_channels=self.mid_channels,
                    out_channels=self.out_channels,
                    kernel_size=self.kernel_size,
                    stride=self.stride,
                    padding=self.padding,
                    dilation=_dilation,
                    groups=self.groups,
                    bias=self.bias,
                    act=Activation.Identity,
                    is_folded=is_folded,
                    in_shape=in_shape,
                    out_shape=out_shape
                )
            )
        self.select = StatsSelect(out_channels, len(dilation), null=null, reduction=reduction)
        # self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1. / n))
                # m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm1d):
                if m.weight is not None:
                    m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                # m.weight.data.fill_(1)
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, inputs):
        result = self.conv1(self.act1(self.bn1(inputs)))
        result = self.act2(self.bn2(result))
        # result = self.select([conv(result) for conv in self.conv2])
        result = self.select(self.conv2[0](result), self.conv2[1](result))
        result = torch.cat([inputs, result], 1)
        return result

    @property
    def multiply_adds(self):
        madds = self.bn1.multiply_adds
        madds += self.conv1.multiply_adds
        madds = self.bn2.multiply_adds
        madds += sum([conv.multiply_adds for conv in self.conv2])
        return madds

    @property
    def params(self):
        params = self.bn1.params
        params += self.conv1.params
        params += self.bn2.params
        params += sum([conv.params for conv in self.conv2])
        return params


if __name__ == "__main__":
    from torchsummary import summary

    model = DtdnnssBase(num_class=7232, feature_dim=128).to('cuda')
    print(sum(p.numel() for p in model.parameters() if p.requires_grad))
    summary(model, (30, 200))

